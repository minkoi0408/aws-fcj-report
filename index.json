[{"uri":"https://workshop-sample.fcjuni.com/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Huỳnh Vũ Minh Khôi\nPhone Number: 0938462246\nEmail: Khoihvmse182393@fpt.edu.vn\nUniversity: FPT University HCM\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/09/2025 to 8/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.1-event1/","title":"Event 1: Vietnam Cloud Day 2025 - Keynote Address","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nEvent Report “Vietnam Cloud Day 2025: Keynote Address” Time and Theme Time: Thursday, September 18, 2025 (9:00 AM – 12:00 PM) Theme: Keynote Address \u0026amp; Panel Discussion: Navigating the GenAI Revolution Event Objective Provide strategic direction from the Government, AWS, and major enterprises on the vision for Cloud and AI in Vietnam. Share experiences and challenges faced by Executives when adopting and managing the change brought about by Generative AI (GenAI) within their organizations. Offer an overview of the role of Cloud technology in accelerating business innovation. List of Speakers/Panelists Eric Yeo - Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS (Keynote) Jaime Valles - Vice President, General Manager Asia Pacific and Japan, AWS (AWS Keynote) Dr. Jens Lottner - CEO, Techcombank (Customer Keynote 1) Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network (Customer Keynote 2) Jeff Johnson - Managing Director, ASEAN, AWS (Moderator Panel) Vu Van - Co-founder \u0026amp; CEO, ELSA Corp (Panelist) Nguyen Hoa Binh - Chairman, Nexttech Group (Panelist) Dieter Botha - CEO, TymeX (Panelist) Highlights of the Content 1. Keynote Address \u0026amp; Customer Keynotes National and AWS Vision: Listen to important speeches from Government representatives and senior AWS leadership regarding the role of cloud computing and AI in digital economic growth. Enterprise Case Study: Learn from the digital transformation experience and adoption of new technology by major enterprises like Techcombank and U2U Network. 2. Panel discussion: Navigating the GenAI Revolution: Strategies for Executive Leadership GenAI Strategy: Leaders discuss how to steer organizations through the rapid advancements of GenAI. Innovation Culture: Sharing on fostering an innovation culture, aligning AI initiatives with business goals, and managing organizational change associated with AI integration. Key Takeaways Leadership Mindset \u0026amp; Strategy GenAI as an Executive Priority: GenAI is not just a tool for the technical team but a strategic issue at the executive level, requiring top-down alignment with business objectives. Learning from Leaders: Gained valuable insights from CEOs on how they balance the pursuit of new technology (AI) with maintaining core business stability and growth. Networking and Vision Expansion Holistic View: Gained a comprehensive view of Cloud and AI technology trends at the national and regional levels. Understanding Challenges: Understood the practical challenges (governance, culture, investment) that large organizations face when deploying new technology. Application to Work Technology Alignment: Apply the strategic mindset learned to align AWS tools and services (such as Bedrock/SageMaker) with specific business objectives of the project/company. Value Proposition: Use metrics and case studies from the Keynote to create clearer value propositions when discussing Cloud technology with stakeholders. Event Experience Attending the Vietnam Cloud Day 2025 Keynote session provided a unique experience in terms of scale and strategic level.\nInspiring Environment: Witnessed leading technology executives in Vietnam and the region discussing the future of AI firsthand. Informational Value: The Keynote presentations, especially from the CEOs, offered practical insights and lessons on leadership in a rapidly evolving technological environment. Overall, this event helped me enhance my strategic vision and understand the complexity of integrating AI into core business decisions.\n"},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/","title":"Worklog","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nIn this section you will need to introduce your worklog, how it is structured, how many weeks the program lasted, and what you did during those weeks.\nTypically, and as a standard, a worklog is completed over approximately 3 months (during the internship period) with the following content for each week:\nWeek 1: Familiarization with AWS and basic services in AWS\nWeek 2: Deploying EC2, practicing IAM Roles and advanced AWS CLI\nWeek 3: Mastering VPC, Subnets, Internet Gateway, and SG/NACL security\nWeek 4: Exploring Amazon S3: Storage Classes, Versioning, and Static Website Hosting\nWeek 5: Deploying Amazon RDS, configuring Multi-AZ and Read Replicas\nWeek 6: Working with DynamoDB (NoSQL) and ElastiCache (Caching Layer)\nWeek 7: Configuring Auto Scaling Group (ASG), ALB, and monitoring with CloudWatch\nWeek 8: Building Serverless architecture with AWS Lambda and API Gateway\nWeek 9: Handling Data Streaming with Amazon Kinesis and ETL pipeline with AWS Glue\nWeek 10: Integrating asynchronous applications using SQS, SNS, and Step Functions\nWeek 11: Implementing advanced security: AWS WAF, Shield, and GuardDuty\nWeek 12: Reviewing all knowledge, resource cleanup, and certification guidance\n"},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.1-blog1/","title":"Blog 1: Evolving the Healthcare Data Lake with Microservices","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices A data lake is a centralized, secure repository that stores all data (raw and processed) for analysis, helping healthcare providers break down silos, gain business insights, and protect patient privacy. This post details the architectural evolution of a healthcare data lake solution toward a microservices-based approach.\nArchitecture Guidance: From Monolith to Microservices The primary design shift involved decomposing a single service into smaller, specialized microservices. This was done to enhance maintainability and flexibility, especially when integrating diverse healthcare data formats (like HL7v2). Connectors are now encapsulated separately, allowing independent modification. The microservices are loosely coupled via a publish/subscribe model centered around a \u0026ldquo;pub/sub hub.\u0026rdquo;\n[Image of Microservices Architecture communicating through a Message Broker]\nThe current scope focuses on the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe microservices approach ensures reusability and autonomy, specializing each service to do one thing well, often implemented using an event-driven architecture.\nMicroservice Boundary Decisions Boundaries for microservices were drawn based on intrinsic, extrinsic, and human factors:\nCategory Consideration Examples Intrinsic Technology stack, performance requirements, scalability needs Extrinsic Dependent functionality, rate of change, potential for reuse Human Team ownership, managing cognitive load across development teams Technology Choices and Communication Scope Key technologies considered for inter-service communication:\nCommunication scope Technologies / patterns used Within a microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices (Single service) AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services (Organizational) Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub and Asynchronous Communication A hub-and-spoke architecture using a message broker (the Pub/Sub Hub) is ideal for a small number of tightly related microservices. This design:\nReduces coupling, as each microservice depends only on the hub. Limits inter-microservice connections to the message content. Reduces synchronous calls since the pub/sub model is an asynchronous push. Drawback: Requires robust coordination and monitoring to prevent services from incorrectly processing messages not intended for them.\nCore Microservices Implementation 1. Core Microservice This service provides the foundational data and communication layer:\nData Storage: Uses Amazon S3 for the data lake and Amazon DynamoDB for the data catalog. Ingestion Logic: AWS Lambda functions are responsible for writing messages into the data lake and catalog. Communication Hub: An Amazon SNS topic acts as the central hub. 2. Front Door Microservice This layer handles external interaction and security:\nProvides an API Gateway for external REST interaction. Authentication/Authorization: Managed via OIDC using Amazon Cognito. Deduplication: A self-managed deduplication mechanism using DynamoDB is implemented instead of relying on SNS FIFO limitations (5-minute TTL, mandatory SQS FIFO). 3. Staging ER7 Microservice Responsible for data format conversion:\nA Lambda \u0026ldquo;trigger\u0026rdquo; subscribes to the pub/sub hub, using attribute filtering. Uses a Step Functions Express Workflow to execute the conversion logic (ER7 → JSON). The workflow uses two Lambdas: one for fixing ER7 formatting (newlines) and one for parsing logic. Results (or errors) are pushed back into the pub/sub hub for downstream processing. New Features and Security Enhancements 1. AWS CloudFormation Cross-Stack References This feature enables logical separation and reusability by allowing different CloudFormation stacks (e.g., Front Door) to securely reference and consume resources (like the SNS Topic or S3 Bucket) created and Exported by the Core Microservice stack.\nExample outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ... "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.2-blog2/","title":"Blog 2: DNS Log Analysis and Allowlist Construction","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance: From Monolith to Microservices The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.” [Image of Microservices Architecture communicating through a Message Broker]\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe microservices approach ensures reusability and autonomy, specializing each service to do one thing well, often implemented in an event-driven architecture.\nMicroservice Boundary Decisions When determining where to draw boundaries between microservices, consider:\nCategory Consideration Examples Intrinsic Technology used, performance, reliability, scalability Extrinsic Dependent functionality, rate of change, reusability Human Team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub - Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub - Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito - Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON [Image of AWS Step Functions Express Workflow converting HL7 ER7 to JSON] Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References This feature enables logical separation and reusability by allowing different CloudFormation stacks to securely reference and consume resources created and Exported by the Core Microservice stack.\nExample outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/3.3-blog3/","title":"Blog 3: Detecting Exceptional Traffic using CloudWatch Alarms and Lambda","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance: From Monolith to Microservices The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\n[Image of Microservices Architecture communicating through a Message Broker]\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe microservices approach ensures reusability and autonomy, specializing each service to do one thing well, often implemented in an event-driven architecture.\nMicroservice Boundary Decisions When determining where to draw boundaries between microservices, consider:\nCategory Consideration Examples Intrinsic Technology used, performance, reliability, scalability Extrinsic Dependent functionality, rate of change, reusability Human Team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub - Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub - Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito - Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. Automated Detection and Response to Exceptional Traffic (CloudWatch \u0026amp; Lambda) This feature enables advanced observability and automates the incident response workflow by combining Amazon CloudWatch and AWS Lambda.\nAnomaly Detection CloudWatch Alarms: Used to monitor key metrics (e.g., API error counts, network traffic) in real-time. Instead of static thresholds, the CloudWatch Anomaly Detection feature creates an ML model of the metric’s normal behavior, automatically adjusting the alert boundaries. Goal: To flag truly \u0026ldquo;exceptional events\u0026rdquo; that fall outside the expected historical behavior, such as a sudden spike in errors or an unexpected drop in traffic. Automated Response When the CloudWatch Alarm transitions to the ALARM state, it triggers an AWS Lambda function as the action target. Lambda Functionality: The Lambda performs automated response tasks to reduce Mean Time to Resolution (MTTR), including: Enhanced Notification: Sending rich messages via Amazon SNS to channels like Slack or PagerDuty. Diagnostic Collection: Calling other AWS APIs to gather contextual logs or system state information around the time of the anomaly. Basic Remediation: Executing simple corrective steps, such as throttling requests to a specific resource or recycling a unresponsive component. 2. AWS CloudFormation Cross-Stack References This feature enables logical separation and reusability by allowing different CloudFormation stacks to securely reference and consume resources created and Exported by the Core Microservice stack.\nExample outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Creating Gateway Endpoint for S3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll configure a Gateway VPC Endpoint to enable private connectivity between your VPC and Amazon S3. This eliminates the need for internet gateway, NAT devices, or VPN connections, while keeping traffic within the AWS network.\nWhat is a Gateway Endpoint? Gateway endpoints are route-based endpoints that appear as targets in your VPC route tables. When you create a gateway endpoint for S3 or DynamoDB, AWS automatically updates the specified route tables with routes directing traffic to the endpoint. This approach:\nIncurs no additional data transfer charges Doesn\u0026rsquo;t require elastic network interfaces (ENIs) Scales automatically to handle your workload Benefits:\nCost Efficiency: No NAT Gateway charges for S3 access Security: Traffic never leaves the AWS network Performance: Lower latency compared to internet-based access Simplified Architecture: No need to manage NAT or internet gateway for S3 access Step 1: Navigate to VPC Endpoints Console Access the Amazon VPC Console in the us-east-1 region From the left navigation menu, locate and click on Endpoints Click the Create endpoint button in the upper right Expected Observation: You should see 6 pre-existing VPC endpoints in the list. These are Interface Endpoints for AWS Systems Manager (SSM) services that were automatically provisioned by the CloudFormation template during setup. These SSM endpoints enable secure, private connectivity to Systems Manager without requiring public internet access, allowing you to manage EC2 instances through Session Manager.\nStep 2: Configure Endpoint Basic Settings In the Create endpoint configuration page:\nName tag: Enter a descriptive name: s3-gateway-endpoint\nThis helps identify the endpoint\u0026rsquo;s purpose in a multi-endpoint environment Service category: Select AWS services\nThis option allows you to choose from AWS-managed service endpoints Step 3: Select the S3 Gateway Service In the Services section, use the search functionality:\nType s3 in the filter box The list will display S3-related endpoint options Important: Select the service entry where:\nService Name contains com.amazonaws.us-east-1.s3 Type column shows Gateway Do NOT select the Interface type endpoint for S3.\nUnderstanding the Difference:\nGateway endpoint (what we\u0026rsquo;re creating): Uses route table entries, free data transfer Interface endpoint: Uses ENIs, incurs PrivateLink charges Step 4: Associate VPC and Route Tables VPC Selection:\nFrom the VPC dropdown menu, select VPC Cloud This is the VPC where your cloud resources reside Route Tables Configuration:\nIn the Route tables section, you\u0026rsquo;ll see all route tables associated with the selected VPC Select the route table that shows association with 2 subnets Important: This is NOT the main route table of the VPC. It\u0026rsquo;s a custom route table created by CloudFormation specifically for the private subnets where your EC2 instances are running. The gateway endpoint will automatically add a route to this table directing S3 traffic (pl-63a5400a prefix list) to the endpoint.\nWhat Happens Behind the Scenes: When you select the route table, AWS will automatically add an entry like:\nDestination: pl-63a5400a (S3 prefix list) Target: vpce-xxxxx (your gateway endpoint ID) Step 5: Configure Endpoint Policy Policy Type: Leave the default Full access option selected\nThis grants unrestricted access to all S3 operations and buckets through this endpoint.\nNote: In a later section of this workshop, you will learn how to implement restrictive VPC endpoint policies to limit access to specific S3 buckets or operations. This demonstrates the principle of least privilege and shows how endpoint policies provide an additional security layer beyond IAM and bucket policies.\nPolicy Options Explained:\nFull access: Allows all S3 actions on all resources Custom: Lets you define specific allowed/denied actions and resources using JSON policy Step 6: Review and Create Endpoint Tags (Optional): Skip adding tags for this workshop\nIn production, consider adding tags like Environment: Workshop, Owner: YourName Click Create endpoint to provision the gateway endpoint\nConfirmation: You should see a success message indicating the endpoint was created\nClick the X or Close button to return to the endpoints list\nVerification:\nThe new endpoint appears in your endpoints list with status available Check the associated route table to see the new S3 prefix list route Understanding What Was Created Your gateway endpoint is now active and has automatically:\nUpdated the route table with an entry pointing S3 traffic to the endpoint Created a managed prefix list containing all S3 IP ranges for us-east-1 Enabled private S3 access for resources in the associated subnets Next Steps: In the following section, you\u0026rsquo;ll test this endpoint by accessing S3 from an EC2 instance in your VPC and verify that traffic flows through the private endpoint instead of the internet.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Environment Preparation for Hybrid Connectivity","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you\u0026rsquo;ll prepare the environment to simulate hybrid cloud connectivity between an on-premises datacenter and AWS. This setup demonstrates how organizations can securely access AWS services like S3 from their corporate networks through private connections.\nWhat You\u0026rsquo;ll Configure:\nDeploy additional DNS infrastructure using CloudFormation Configure VPN routing to enable on-premises to cloud connectivity Architecture Context: This configuration simulates a real-world hybrid architecture where:\nAn on-premises datacenter connects to AWS via Site-to-Site VPN DNS queries from on-premises are resolved through Route 53 S3 access from on-premises flows through Interface VPC Endpoints All traffic remains private without internet exposure Part 1: Deploy DNS Infrastructure with CloudFormation To enable proper DNS resolution in the hybrid environment, you\u0026rsquo;ll deploy a CloudFormation stack that creates Route 53 resolver endpoints and private hosted zones.\nWhat This Stack Creates:\nThe CloudFormation template provisions three critical DNS components:\nRoute 53 Private Hosted Zone\nHosts DNS records (Alias records) for the S3 Interface Endpoint Allows on-premises systems to resolve S3 endpoint names to private IP addresses Scoped to your VPC for private resolution Route 53 Inbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC Cloud\u0026rdquo; Receives DNS queries from on-premises environments Forwards queries to the Private Hosted Zone for resolution Creates elastic network interfaces (ENIs) in your VPC subnets Route 53 Outbound Resolver Endpoint\nDeployed in \u0026ldquo;VPC On-prem\u0026rdquo; Forwards DNS queries for S3 domains to \u0026ldquo;VPC Cloud\u0026rdquo; Enables conditional DNS forwarding based on domain names Routes queries through the VPN tunnel to the Inbound Resolver DNS Flow Diagram:\nDeployment Steps:\nLaunch CloudFormation Stack:\nClick this link to open the CloudFormation console with pre-configured template: Deploy DNS Infrastructure Stack\nThe template URL is pre-populated and the stack name is set to PLOnpremSetup.\nReview and Accept Defaults:\nAll parameters are pre-configured with optimal values The template will automatically detect your VPC IDs and subnet configurations Scroll to the bottom of the page Acknowledge and Create:\nCheck the acknowledgment box: \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; Click Create stack Monitor Deployment (Optional):\nStack creation typically takes 3-5 minutes You can monitor progress in the Events tab You don\u0026rsquo;t need to wait - proceed to the next section while this deploys Background Processing: The CloudFormation stack creates resolver endpoints which can take a few minutes. The next configuration step (routing table update) can be completed in parallel while CloudFormation runs.\nWhat Gets Created:\n2 Resolver Endpoint ENIs (Inbound in VPC Cloud) 2 Resolver Endpoint ENIs (Outbound in VPC On-prem) 1 Private Hosted Zone for S3 endpoint DNS resolution Resolver rules for conditional DNS forwarding Part 2: Configure VPN Routing for On-Premises Connectivity Now you\u0026rsquo;ll configure the routing table in \u0026ldquo;VPC On-prem\u0026rdquo; to direct traffic destined for the cloud environment through the VPN tunnel.\nUnderstanding the VPN Setup:\nThis workshop uses a software-based VPN solution running on an EC2 instance to simulate the connection between your simulated on-premises environment and AWS:\nVPN Software: strongSwan (open-source IPsec-based VPN) VPN Gateway Instance: An EC2 instance acting as the customer gateway Connection Type: Site-to-Site VPN tunnel Purpose: Simulates a real datacenter\u0026rsquo;s VPN appliance Production Note: In real-world deployments, you would use:\nPhysical or virtual VPN appliances in your datacenter AWS Site-to-Site VPN with redundant connections AWS Transit Gateway for centralized connectivity Multiple VPN tunnels for high availability Configuration Steps:\nStep 1: Identify the VPN Gateway Instance\nOpen the Amazon EC2 Console\nLocate and select the EC2 instance with the name infra-vpngw-test\nThis instance is running strongSwan VPN software It\u0026rsquo;s configured to terminate the VPN tunnel from \u0026ldquo;VPC Cloud\u0026rdquo; In the Details tab (bottom pane), locate the Instance ID\nFormat: i-0123456789abcdef0 Copy this Instance ID to your clipboard or text editor You\u0026rsquo;ll need this ID in the next steps Instance Role: This EC2 instance has IP forwarding enabled and is configured with IPsec tunnels. It acts as the gateway between the simulated on-premises network and AWS Transit Gateway.\nStep 2: Navigate to VPC Route Tables\nUse the search box at the top of the AWS Console Type VPC and press Enter From the VPC Dashboard left menu, click Route Tables Step 3: Update On-Premises Route Table\nLocate the correct route table:\nFilter or search for the route table named RT Private On-prem This is the route table associated with the private subnets in \u0026ldquo;VPC On-prem\u0026rdquo; Click on the route table to select it Access route editing:\nClick on the Routes tab in the bottom details pane Click Edit routes button Add new route entry:\nClick Add route button Configure the new route:\nDestination: Enter your Cloud VPC CIDR block\nFormat: 10.0.0.0/16 (or whatever CIDR your Cloud VPC uses) This tells the route table where cloud resources are located Target: Select Instance, then choose the infra-vpngw-test instance\nPaste the Instance ID you copied earlier Alternatively, start typing the instance name and select from the dropdown Save the configuration: Click Save changes The route table is immediately updated Understanding What You Configured:\nThis route entry tells instances in \u0026ldquo;VPC On-prem\u0026rdquo; that:\nTraffic destined for Cloud VPC CIDR → Send to VPN Gateway Instance Traffic Flow:\nOn-Prem EC2 Instance → Route Table Lookup → VPN Gateway EC2 → IPsec Tunnel → Transit Gateway → VPC Cloud Routing Configured! Now any traffic from \u0026ldquo;VPC On-prem\u0026rdquo; destined for resources in \u0026ldquo;VPC Cloud\u0026rdquo; will be routed through the VPN tunnel. This simulates how your corporate network would route traffic to AWS over a Site-to-Site VPN connection.\nVerification Steps Verify CloudFormation Stack:\nReturn to CloudFormation Console Check that PLOnpremSetup stack shows CREATE_COMPLETE status Click on the stack and view the Resources tab to see created components Verify Route Table:\nGo back to the RT Private On-prem route table Confirm the new route appears with: Destination: Cloud VPC CIDR Target: VPN Gateway instance ID Status: Active What\u0026rsquo;s Next: With DNS infrastructure and routing in place, you\u0026rsquo;re ready to:\nCreate an Interface VPC Endpoint for S3 Test S3 access from the simulated on-premises environment Verify DNS resolution through Route 53 Resolver Confirm all traffic flows through private connections Architecture Summary You\u0026rsquo;ve now established the foundational hybrid connectivity:\nDNS Resolution Path:\nOn-Prem Application → Outbound Resolver Endpoint → VPN Tunnel → Inbound Resolver Endpoint → Private Hosted Zone → S3 Endpoint IP Data Path:\nOn-Prem Application → VPN Gateway → IPsec Tunnel → Transit Gateway → VPC Cloud → Interface Endpoint → S3 This architecture demonstrates AWS best practices for:\nPrivate connectivity to AWS services from on-premises DNS resolution in hybrid environments Secure data transfer without internet exposure Scalable VPN connectivity through Transit Gateway "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Set up development environment (Git, VS Code) 08/09/2025 08/09/2025 FCJ Internal Documentation 3 - Learn about AWS and its types of services + Compute (EC2, Lambda) + Storage (S3, EBS) + Networking (VPC) + Database (RDS, DynamoDB) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + Test basic CLI commands 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + Security Groups - SSH connection methods to EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume + Test basic operations 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood AWS Cloud Computing basics:\nLearned what AWS is and its role as a cloud service provider Explored core service categories: Compute (EC2, Lambda) Storage (S3, EBS) Networking (VPC) Database (RDS, DynamoDB) Security (IAM, Security Groups) Set up AWS account and environment:\nCreated AWS Free Tier account successfully Configured basic security settings Set up billing alerts to monitor costs Got familiar with AWS Management Console:\nLearned to navigate the AWS Console interface Found and accessed different AWS services Understood basic service dashboards Installed and configured AWS CLI:\nInstalled AWS CLI version 2 on local machine Created IAM user credentials Configured AWS CLI with: Access Key ID Secret Access Key Default Region (ap-southeast-1) Verified configuration with basic commands Practiced with AWS CLI commands:\naws sts get-caller-identity - Check account info aws ec2 describe-regions - List AWS regions aws ec2 describe-instances - View EC2 instances aws s3 ls - List S3 buckets aws ec2 describe-key-pairs - Manage SSH keys Learned EC2 basics:\nUnderstood different EC2 instance types (t2, t3, m5) Learned about AMI (Amazon Machine Images) Studied EBS volumes and their types Understood Security Groups for firewall rules Completed EC2 hands-on practice:\nLaunched EC2 instance (t2.micro) Configured Security Group for SSH access Connected to EC2 via SSH Created and attached EBS volume Tested basic Linux commands on EC2 Gained foundational AWS skills:\nCan use both Console and CLI to manage resources Understand basic AWS concepts and terminology Ready to learn more advanced topics in Week 2 "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 2 Objectives: Learn about AWS storage service (S3) and basic concepts. Understand Virtual Private Cloud (VPC) for networking. Study Identity and Access Management (IAM) for security. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create S3 bucket to host static website - Upload HTML/CSS demo files to S3 15/09/2025 15/09/2025 AWS Journey 3 - Enable Static Website Hosting feature in S3 - Configure bucket policy for public read access - Test website access via S3 link 16/09/2025 16/09/2025 AWS Journey 4 - Create RDS MySQL instance (Free Tier) - Configure VPC security group to allow DB connection - Note endpoint \u0026amp; credentials 17/09/2025 17/09/2025 AWS Journey 5 - Create EC2 instance, install MySQL client - Connect from EC2 to RDS via command line - Test creating database \u0026amp; simple table 18/09/2025 18/09/2025 AWS Journey 6 - Learn about Route53, create hosted zone - Add A/CNAME records to point domain to S3 static site - Test website access via domain 19/09/2025 19/09/2025 AWS Journey Week 2 Achievements: Learned about Amazon S3 storage:\nUnderstood S3 as object storage service for files and data Learned about different storage classes: S3 Standard - for frequently accessed data S3 Glacier - for archival and backup Studied bucket policies and access control Learned about versioning for data protection Practiced with S3:\nCreated S3 buckets with unique names Uploaded and downloaded files using Console and CLI: aws s3 cp file.txt s3://my-bucket/ aws s3 ls s3://my-bucket/ Configured bucket policies for access control Enabled versioning on buckets Set up simple static website hosting Tested file management operations Understood VPC networking basics:\nLearned VPC as virtual network in AWS Studied CIDR blocks and IP addressing Understood difference between public and private subnets: Public subnet - has internet access Private subnet - no direct internet access Learned about VPC components: Internet Gateway - for internet connectivity Route Tables - for traffic routing Security Groups - instance-level firewall NACLs - subnet-level firewall Practiced with VPC:\nCreated custom VPC with CIDR block Created public and private subnets Configured route tables for subnets Attached Internet Gateway to VPC Launched EC2 instance in custom VPC Configured Security Groups for access control Tested network connectivity Learned IAM security basics:\nUnderstood IAM for access management Learned about IAM components: Users - individual accounts Groups - collections of users Roles - for AWS services Policies - permission definitions Studied IAM best practices: Use MFA for security Follow least privilege principle Create separate users instead of using root Practiced with IAM:\nCreated IAM users and groups Attached policies to users and groups Created custom policies for specific permissions Set up IAM roles for EC2 Enabled MFA for users Tested user permissions and access Gained practical AWS skills:\nCan manage storage with S3 Understand basic networking with VPC Know how to control access with IAM Ready for more advanced topics in Week 3 "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Learn about Amazon RDS for managed databases. Understand AWS Lambda and serverless basics. Study CloudWatch for monitoring and logging. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to CDN concepts and CloudFront benefits - Create CloudFront Distribution to distribute S3 website content 22/09/2025 22/09/2025 AWS Journey 3 - Configure CloudFront behavior and cache policy - Test website access via CloudFront URL - Perform invalidation to update new content 23/09/2025 23/09/2025 AWS Journey 4 - Introduction to DynamoDB (NoSQL Database) - Create DynamoDB tables (Users, Products, etc.) - Practice CRUD operations on Console 24/09/2025 24/09/2025 AWS Journey 5 - Connect and query DynamoDB using AWS CLI - Write small scripts to add and read data from tables 25/09/2025 25/09/2025 AWS Journey 6 - Learn about ElastiCache (Redis \u0026amp; Memcached) - Create basic Redis cluster - Test connection from EC2 to store and read cache data 26/09/2025 26/09/2025 AWS Journey Week 3 Achievements: Learned about Amazon RDS:\nUnderstood RDS as managed database service Learned benefits of managed databases: Automated backups Automatic updates Easy scaling Studied database engines: MySQL - popular for web applications PostgreSQL - advanced features Understood Multi-AZ for high availability Learned about Read Replicas for scaling Practiced with RDS:\nCreated RDS MySQL instance (db.t3.micro) Configured security groups for database access Connected to RDS from EC2 instance: mysql -h mydb.ap-southeast-1.rds.amazonaws.com -u admin -p Created sample database and tables Configured automated backups Tested snapshot creation and restore Monitored database metrics in CloudWatch Understood AWS Lambda basics:\nLearned serverless computing concept: No server management Auto scaling Pay per use Understood Lambda function structure: Handler function Event input Execution role Studied Lambda triggers: S3 events API Gateway CloudWatch Events Learned Lambda limitations: 15-minute timeout Memory limits Practiced with Lambda:\nCreated Lambda function in Python: def lambda_handler(event, context): return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Hello from Lambda!\u0026#39; } Configured IAM execution role Set up S3 trigger for image processing Tested function execution Monitored logs in CloudWatch Analyzed function performance Learned CloudWatch monitoring:\nUnderstood CloudWatch components: Metrics - performance data Logs - application logs Alarms - automated alerts Learned about metric types: EC2 metrics (CPU, disk, network) RDS metrics (connections, storage) Lambda metrics (invocations, errors) Practiced with CloudWatch:\nCreated alarms for EC2 instances: High CPU usage Low disk space Set up alarms for RDS: High connections Low storage Viewed and analyzed CloudWatch Logs Built simple dashboard for monitoring Configured log retention policies Gained practical experience:\nCan set up and manage RDS databases Understand serverless with Lambda Know how to monitor with CloudWatch Ready for Week 4 advanced topics "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Learn about AWS Lambda and serverless computing. Understand API Gateway for building REST APIs. Study SNS and SQS for messaging services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor) - Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey 3 - Practice creating Replication Instance in DMS - Configure source data (on-premise) and target (RDS) - Perform test data migration 30/09/2025 30/09/2025 AWS Journey 4 - Introduction to Elastic Disaster Recovery (EDR) - Learn how to set up replication server and recovery instance 01/10/2025 01/10/2025 AWS Journey 5 - Practice simulating failures: shut down main EC2 and start recovery instance from EDR - Evaluate recovery time (RTO/RPO) 02/10/2025 02/10/2025 AWS Journey 6 - Create basic Disaster Recovery plan (backup, restore, failover) - Write documentation summarizing Migration + DR processes - Summarize Week 4 knowledge 03/10/2025 03/10/2025 AWS Journey Week 4 Achievements: Learned about serverless computing and benefits:\nNo server management required Pay only for execution time Automatic scaling Created Lambda functions using Python:\nBasic hello world function Function with environment variables Tested with different event sources Configured Lambda settings:\nTimeout (default 3 seconds) Memory allocation Execution role with basic permissions Built REST API with API Gateway:\nCreated simple GET/POST endpoints Connected API to Lambda backend Tested API calls using browser and Postman Practiced with SNS:\nCreated SNS topic Added email subscription Sent test notifications Worked with SQS:\nCreated standard queue Sent messages to queue Retrieved and deleted messages from queue Understood message retention "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Learn about container services on AWS. Understand ECS and basic Docker concepts. Study Load Balancer for distributing traffic. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Infrastructure as Code (IaC) concepts and benefits compared to manual deployment - Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey 3 - Write CloudFormation template to deploy S3 bucket and EC2 instance - Create, update, and delete stack via AWS Console 07/10/2025 07/10/2025 AWS Journey 4 - Introduction to AWS CDK (Cloud Development Kit) - Install AWS CDK, create CDK project using Python or TypeScript - Write CDK code to deploy EC2 instance 08/10/2025 08/10/2025 AWS Journey 5 - Introduction to AWS Systems Manager (SSM) and key features: Parameter Store, Run Command, Automation, Session Manager - Create Parameter Store to store configuration variables 09/10/2025 09/10/2025 AWS Journey 6 - Practice creating Automation Document in SSM to automatically start/stop EC2 - Test Session Manager (access EC2 without SSH key) - Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey Week 5 Achievements: Learned about containerization and Docker:\nUnderstood benefits of containers Learned difference between containers and VMs Studied Docker images and containers Practiced with Docker locally:\nInstalled Docker Desktop Pulled images from Docker Hub Ran basic containers: docker pull nginx docker run -d -p 80:80 nginx Learned basic Docker commands Understood Amazon ECS:\nLearned ECS as managed container service Studied ECS components: Clusters Task definitions Services Compared Fargate (serverless) vs EC2 launch types Deployed application on ECS:\nCreated ECS cluster Created task definition with simple web app Deployed service on Fargate Viewed container logs in CloudWatch Worked with Application Load Balancer:\nCreated ALB for distributing traffic Configured target groups Integrated ALB with ECS service Tested load balancing with multiple tasks "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Learn about monitoring and logging with CloudWatch. Understand Auto Scaling for automatic scaling resources. Study cost optimization and billing management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review basic IAM knowledge - Learn advanced IAM Policy (JSON structure, Effect, Action, Resource, Condition) - Create custom policies and attach to users/groups 13/10/2025 13/10/2025 AWS Journey 3 - Introduction to AWS Key Management Service (KMS) - Create a Customer Managed Key (CMK) and test file encryption/decryption - Apply KMS to encrypt an S3 bucket or an EBS volume 14/10/2025 14/10/2025 AWS Journey 4 - Get familiar with AWS Secrets Manager - Create a secret to store Database connection information - Write a small Lambda script to read the secret from Secrets Manager 15/10/2025 15/10/2025 AWS Journey 5 - Explore AWS Billing Dashboard and Cost Explorer - View costs by service, region, and time period - Set up Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey 6 - Create an AWS Budget and configure cost alerts via email - Write a weekly cost summary report with optimization proposals (stop EC2, cleanup EBS, reduce log retention, etc.) - Wrap up Week 6 learnings 17/10/2025 17/10/2025 AWS Journey Week 6 Achievements: Learned about CloudWatch monitoring:\nUnderstood CloudWatch metrics for AWS services Learned how to view and analyze metrics Studied CloudWatch dashboards for visualization Created CloudWatch alarms:\nSet up alarm for EC2 CPU utilization \u0026gt; 80% Configured SNS topic for email notifications Tested alarm by increasing CPU load Viewed CloudWatch Logs for Lambda executions Understood Auto Scaling concepts:\nLearned benefits of automatic scaling Studied Auto Scaling Groups components Understood scaling policies: Target tracking (maintain metric at target) Step scaling (scale based on thresholds) Practiced with Auto Scaling:\nCreated Launch Template for EC2 instances Set up Auto Scaling Group (min: 2, max: 5) Configured scaling policy based on CPU Generated load to test automatic scaling Observed instances launching and terminating Learned about AWS billing:\nExplored AWS Billing Dashboard Set up billing alerts for cost threshold Reviewed Cost Explorer for spending patterns Learned about cost allocation tags "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Learn about containers and Docker basics. Understand Amazon ECS and container orchestration. Deploy containerized applications on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AWS Well-Architected Framework, 5 pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization - Identify the role and importance of each pillar in system design 27/10/2025 27/10/2025 AWS Journey 3 - Review Secure Architecture Design → IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager 28/10/2025 28/10/2025 AWS Journey 4 - Review Resilient Architecture Design → Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey 5 - Review Performance and Cost Optimization (High-Performing \u0026amp; Cost-Optimized Architectures) → EC2 Auto Scaling, Lambda, Fargate, CloudFront, Global Accelerator, Cost Explorer, Budgets, Savings Plans, Storage Tiering 30/10/2025 30/10/2025 AWS Journey 6 - Comprehensive Practice: + Build a sample architecture combining EC2, S3, RDS, IAM, VPC, CloudFront, Lambda, CloudWatch + Evaluate according to 5 Well-Architected Framework criteria + Write weekly summary report 31/10/2025 31/10/2025 AWS Journey Week 8 Achievements: Learned about containerization:\nUnderstood benefits of containers vs VMs Learned Docker basics and terminology Studied container use cases Practiced with Docker locally:\nInstalled Docker Desktop Pulled images from Docker Hub (nginx, node, python) Created simple Dockerfile: FROM node:14 WORKDIR /app COPY package*.json ./ RUN npm install COPY . . EXPOSE 3000 CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] Built and ran containers locally Understood Amazon ECS:\nLearned ECS architecture and components Compared Fargate (serverless) vs EC2 launch types Studied task definitions and services Worked with ECR:\nCreated ECR repository Tagged and pushed Docker image to ECR Configured authentication with ECR Deployed on ECS:\nCreated ECS cluster Created task definition with container settings Deployed service on Fargate Configured networking and security groups Accessed application via public IP Viewed container logs in CloudWatch "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":" title: \u0026ldquo;Week 9 Worklog\u0026rdquo; date: \u0026ldquo;r Sys.Date()\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 1.9. \u0026quot; Week 9 Objectives: Learn about serverless application development on AWS. Understand AWS Lambda and event-driven architecture. Build and deploy serverless applications. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Data \u0026amp; Analytics ecosystem on AWS - Understand Data Lake concepts, ETL pipeline, and how to connect data from multiple sources 03/11/2025 03/11/2025 AWS Journey 3 - Create Data Lake on Amazon S3 - Directory structure, access permissions - Set up AWS Glue Crawler to identify data schema 04/11/2025 04/11/2025 AWS Journey 4 - Practice AWS Athena to query data in Data Lake - Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey 5 - Introduction and practice with Amazon QuickSight - Connect QuickSight with Athena to visualize data - Create simple dashboard with charts and summary tables 06/11/2025 06/11/2025 AWS Journey 6 - Review \u0026amp; consolidate weekly knowledge: + Data collection → processing → analysis process on AWS + Compare Glue, Athena, QuickSight with traditional tools + Write summary report of practice process 07/11/2025 07/11/2025 AWS Journey Week 9 Achievements: Learned about serverless computing:\nUnderstood serverless benefits (no server management, auto-scaling) Learned AWS Lambda pricing model (pay per execution) Studied serverless use cases Practiced with AWS Lambda:\nCreated Lambda functions in Python and Node.js Configured function settings: Memory: 128MB - 512MB Timeout: 3 seconds - 30 seconds Environment variables for configuration Tested functions with sample events Viewed execution logs in CloudWatch Worked with Lambda triggers:\nSet up S3 trigger for file processing Created EventBridge rule for scheduled execution Configured API Gateway integration Tested automatic function invocation Used AWS SAM:\nInstalled SAM CLI Created SAM project with template.yaml Defined Lambda functions and API Gateway in SAM Deployed application with sam deploy Tested local development with sam local Built serverless API:\nCreated REST API with API Gateway Implemented CRUD operations with Lambda Connected to DynamoDB for data storage Tested API with Postman Configured CORS for web access Integrated with DynamoDB:\nCreated DynamoDB table Implemented Lambda functions to read/write data Used boto3 (Python) for DynamoDB operations Tested data persistence Monitored DynamoDB metrics │ S3: Raw data, all formats │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Processing Layer │ │ - AWS Glue ETL Jobs │ │ - Glue Data Quality │ │ - EMR for complex transformations │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Silver Layer (Cleansed Zone) │ │ S3: Parquet, validated, standardized │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Aggregation Layer │ │ - Glue ETL Jobs (aggregations) │ │ - Athena CTAS queries │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Gold Layer (Business Zone) │ │ S3: Optimized data marts │ └─────────────────────────────────────────┘ ↓ ┌─────────────────────────────────────────┐ │ Consumption Layer │ │ - Amazon Athena (SQL analytics) │ │ - QuickSight (BI dashboards) │ │ - SageMaker (ML models) │ │ - Redshift Spectrum (DW integration) │ └─────────────────────────────────────────┘ Data Governance with AWS Lake Formation:\nAccess Control:\nCentralized permissions management Column-level security for PII fields Row-level security based on user attributes Tag-based access control (TBAC) Cross-account sharing with AWS RAM Permissions Matrix:\nRole Bronze Silver Gold PII Columns Data Engineers Read/Write Read/Write Read/Write Masked Data Analysts Read Read Read Masked Data Scientists Read Read Read Masked Business Users No Access No Access Read Denied Compliance Team Read Read Read Unmasked Data Catalog:\n347 tables cataloged 1,245 columns documented 89 data sources registered Schema versioning enabled Automated metadata extraction 2. Enterprise Data Lake Implementation S3 Data Lake Configuration: Bucket Structure:\ndatalake-bronze-prod/ ├── application-logs/ │ └── year=2026/month=01/day=27/ ├── database-cdc/ │ └── source=postgresql/table=orders/ ├── api-data/ │ └── provider=stripe/endpoint=payments/ ├── file-uploads/ │ └── type=customer-data/ └── iot-sensors/ └── device-type=temperature/location=warehouse-1/ datalake-silver-prod/ ├── customers/ │ └── year=2026/month=01/day=27/ ├── orders/ │ └── year=2026/month=01/day=27/ ├── products/ │ └── year=2026/month=01/ └── transactions/ └── year=2026/month=01/day=27/hour=10/ datalake-gold-prod/ ├── sales-analytics/ │ └── report-date=2026-01-27/ ├── customer-360/ │ └── snapshot-date=2026-01-27/ └── product-performance/ └── analysis-period=2026-01/ Security Configuration:\nBucket encryption: SSE-KMS with automatic key rotation Versioning: Enabled on all buckets MFA Delete: Enabled for production buckets Access logging: Enabled to audit bucket Object Lock: Enabled for compliance data (WORM) Block public access: Enforced at bucket and account level VPC Endpoints: S3 access through private network only Cost Optimization:\nS3 Intelligent-Tiering: Automatic tiering for Bronze layer Lifecycle policies: Bronze: 30 days Standard → Glacier Instant Retrieval Silver: 90 days Standard → Glacier Flexible Retrieval Gold: 365 days Standard → Glacier Deep Archive Compression: Parquet with Snappy (70% size reduction) Partitioning: Reduced scan costs by 85% S3 Select: Push-down filtering (60% cost reduction) Storage costs: $450/month for 8.5 TB (down from $1,200 with Standard storage) Data Quality Metrics:\nData freshness: 99.2% within SLA (15 minutes) Data completeness: 98.7% (missing field validation) Data accuracy: 99.5% (business rule validation) Schema compliance: 100% (automated checks) Duplicate rate: 0.3% (post-deduplication) 3. AWS Glue ETL Pipeline Implementation Glue Crawlers Configuration:\nBronze Crawler:\nName: bronze-data-crawler Schedule: Every 6 hours Data sources: All Bronze layer S3 paths Classifiers: JSON, CSV, Parquet, Avro custom classifiers Tables created: 45 tables Partitions discovered: 2,834 partitions Schema inference: Automatic with conflict resolution Silver Crawler:\nName: silver-data-crawler Schedule: Daily at 2 AM Data sources: Silver layer S3 paths Partition projection: Enabled for time-based partitions Tables created: 28 tables Schema evolution: Track and version changes Glue ETL Jobs:\nJob 1 - Bronze to Silver Transformation:\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglue.dynamicframe import DynamicFrame from pyspark.sql.functions import * args = getResolvedOptions(sys.argv, [\u0026#39;JOB_NAME\u0026#39;, \u0026#39;database_name\u0026#39;, \u0026#39;table_name\u0026#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[\u0026#39;JOB_NAME\u0026#39;], args) # Read from Bronze layer bronze_dyf = glueContext.create_dynamic_frame.from_catalog( database=args[\u0026#39;database_name\u0026#39;], table_name=args[\u0026#39;table_name\u0026#39;], transformation_ctx=\u0026#34;bronze_dyf\u0026#34; ) # Convert to Spark DataFrame for transformations df = bronze_dyf.toDF() # Data Quality Checks df = df.filter(col(\u0026#34;order_id\u0026#34;).isNotNull()) # Remove null order IDs df = df.filter(col(\u0026#34;order_amount\u0026#34;) \u0026gt; 0) # Valid amounts only df = df.dropDuplicates([\u0026#34;order_id\u0026#34;]) # Deduplicate # Data Transformations df = df.withColumn(\u0026#34;order_date\u0026#34;, to_date(col(\u0026#34;created_at\u0026#34;))) df = df.withColumn(\u0026#34;order_amount\u0026#34;, col(\u0026#34;order_amount\u0026#34;).cast(\u0026#34;decimal(10,2)\u0026#34;)) df = df.withColumn(\u0026#34;customer_email\u0026#34;, lower(trim(col(\u0026#34;customer_email\u0026#34;)))) # PII Masking df = df.withColumn(\u0026#34;customer_phone_masked\u0026#34;, regexp_replace(col(\u0026#34;customer_phone\u0026#34;), \u0026#34;\\\\d{4}$\u0026#34;, \u0026#34;XXXX\u0026#34;)) # Add metadata columns df = df.withColumn(\u0026#34;processing_timestamp\u0026#34;, current_timestamp()) df = df.withColumn(\u0026#34;data_source\u0026#34;, lit(\u0026#34;order_system\u0026#34;)) # Add partitioning columns df = df.withColumn(\u0026#34;year\u0026#34;, year(col(\u0026#34;order_date\u0026#34;))) df = df.withColumn(\u0026#34;month\u0026#34;, month(col(\u0026#34;order_date\u0026#34;))) df = df.withColumn(\u0026#34;day\u0026#34;, dayofmonth(col(\u0026#34;order_date\u0026#34;))) # Convert back to DynamicFrame silver_dyf = DynamicFrame.fromDF(df, glueContext, \u0026#34;silver_dyf\u0026#34;) # Write to Silver layer in Parquet format with partitioning glueContext.write_dynamic_frame.from_options( frame=silver_dyf, connection_type=\u0026#34;s3\u0026#34;, connection_options={ \u0026#34;path\u0026#34;: \u0026#34;s3://datalake-silver-prod/orders/\u0026#34;, \u0026#34;partitionKeys\u0026#34;: [\u0026#34;year\u0026#34;, \u0026#34;month\u0026#34;, \u0026#34;day\u0026#34;] }, format=\u0026#34;parquet\u0026#34;, format_options={ \u0026#34;compression\u0026#34;: \u0026#34;snappy\u0026#34; }, transformation_ctx=\u0026#34;silver_write\u0026#34; ) job.commit() Job 2 - Silver to Gold Aggregation:\n# Daily sales aggregation job from pyspark.sql.functions import sum, count, avg, max, min from pyspark.sql.window import Window # Read from Silver layer orders_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/orders/\u0026#34;) customers_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/customers/\u0026#34;) products_df = spark.read.parquet(\u0026#34;s3://datalake-silver-prod/products/\u0026#34;) # Join datasets sales_df = orders_df \\ .join(customers_df, \u0026#34;customer_id\u0026#34;, \u0026#34;left\u0026#34;) \\ .join(products_df, \u0026#34;product_id\u0026#34;, \u0026#34;left\u0026#34;) # Daily sales aggregation daily_sales = sales_df.groupBy(\u0026#34;order_date\u0026#34;, \u0026#34;product_category\u0026#34;) \\ .agg( sum(\u0026#34;order_amount\u0026#34;).alias(\u0026#34;total_revenue\u0026#34;), count(\u0026#34;order_id\u0026#34;).alias(\u0026#34;total_orders\u0026#34;), avg(\u0026#34;order_amount\u0026#34;).alias(\u0026#34;avg_order_value\u0026#34;), countDistinct(\u0026#34;customer_id\u0026#34;).alias(\u0026#34;unique_customers\u0026#34;) ) # Calculate running totals with window functions window_spec = Window.partitionBy(\u0026#34;product_category\u0026#34;) \\ .orderBy(\u0026#34;order_date\u0026#34;) \\ .rowsBetween(Window.unboundedPreceding, Window.currentRow) daily_sales = daily_sales.withColumn( \u0026#34;cumulative_revenue\u0026#34;, sum(\u0026#34;total_revenue\u0026#34;).over(window_spec) ) # Write to Gold layer daily_sales.write \\ .mode(\u0026#34;overwrite\u0026#34;) \\ .partitionBy(\u0026#34;order_date\u0026#34;) \\ .parquet(\u0026#34;s3://datalake-gold-prod/sales-analytics/daily-sales/\u0026#34;) Glue Data Quality Rules:\nRules = [ # Completeness checks ColumnExists \u0026#34;order_id\u0026#34;, ColumnExists \u0026#34;customer_id\u0026#34;, ColumnExists \u0026#34;order_amount\u0026#34;, # Uniqueness checks IsUnique \u0026#34;order_id\u0026#34;, # Validity checks ColumnValues \u0026#34;order_amount\u0026#34; \u0026gt; 0, ColumnValues \u0026#34;order_status\u0026#34; in [\u0026#34;pending\u0026#34;, \u0026#34;completed\u0026#34;, \u0026#34;cancelled\u0026#34;], ColumnDataType \u0026#34;order_date\u0026#34; = \u0026#34;date\u0026#34;, # Consistency checks ColumnLength \u0026#34;customer_email\u0026#34; \u0026lt;= 255, RowCount \u0026gt; 0, # Custom rules CustomSql \u0026#34;SELECT COUNT(*) FROM primary WHERE order_amount \u0026gt; 100000\u0026#34; = 0 ] Glue Job Monitoring:\nJob execution metrics tracked in CloudWatch Average job duration: 8 minutes (processing 4.5GB data) Success rate: 99.1% Failed jobs: Auto-retry 3 times with exponential backoff Cost per job run: $0.65 (2 DPU for 8 minutes) Jobs triggered: 124 executions this week Data processed: 562 GB total 4. Advanced Analytics with Amazon Athena Athena Workgroup Configuration:\nProduction Workgroup:\nData scanned limit: 10 TB per month Query timeout: 30 minutes Results encryption: SSE-KMS Results location: s3://athena-results-prod/ Results retention: 30 days Engine version: Athena engine version 3 Cost allocation tags: Environment=Production, Team=Analytics Development Workgroup:\nData scanned limit: 1 TB per month Query timeout: 10 minutes Enforce workgroup configuration Query result reuse: 60 minutes Advanced SQL Queries Examples:\nQuery 1 - Customer Cohort Analysis:\nWITH customer_cohorts AS ( SELECT customer_id, DATE_TRUNC(\u0026#39;month\u0026#39;, MIN(order_date)) AS cohort_month, DATE_TRUNC(\u0026#39;month\u0026#39;, order_date) AS order_month, SUM(order_amount) AS revenue FROM gold_layer.orders WHERE order_date \u0026gt;= DATE \u0026#39;2025-01-01\u0026#39; GROUP BY 1, 3 ), cohort_metrics AS ( SELECT cohort_month, order_month, DATE_DIFF(\u0026#39;month\u0026#39;, cohort_month, order_month) AS months_since_first_order, COUNT(DISTINCT customer_id) AS customers, SUM(revenue) AS total_revenue FROM customer_cohorts GROUP BY 1, 2 ) SELECT cohort_month, months_since_first_order, customers, total_revenue, ROUND(100.0 * customers / FIRST_VALUE(customers) OVER (PARTITION BY cohort_month ORDER BY months_since_first_order), 2) AS retention_rate FROM cohort_metrics ORDER BY cohort_month, months_since_first_order; Query 2 - Product Affinity Analysis:\nWITH product_pairs AS ( SELECT o1.order_id, o1.product_id AS product_a, o2.product_id AS product_b FROM gold_layer.order_items o1 JOIN gold_layer.order_items o2 ON o1.order_id = o2.order_id AND o1.product_id \u0026lt; o2.product_id ) SELECT pa.product_name AS product_a_name, pb.product_name AS product_b_name, COUNT(*) AS frequency, ROUND(100.0 * COUNT(*) / ( SELECT COUNT(DISTINCT order_id) FROM gold_layer.order_items ), 2) AS support_percentage FROM product_pairs pp JOIN gold_layer.products pa ON pp.product_a = pa.product_id JOIN gold_layer.products pb ON pp.product_b = pb.product_id GROUP BY 1, 2 HAVING COUNT(*) \u0026gt; 50 ORDER BY frequency DESC LIMIT 20; Query 3 - Time Series Forecasting Preparation:\nWITH daily_metrics AS ( SELECT order_date, SUM(order_amount) AS daily_revenue, COUNT(DISTINCT order_id) AS daily_orders, COUNT(DISTINCT customer_id) AS daily_customers, AVG(order_amount) AS avg_order_value FROM gold_layer.orders WHERE order_date \u0026gt;= CURRENT_DATE - INTERVAL \u0026#39;365\u0026#39; DAY GROUP BY order_date ), moving_averages AS ( SELECT order_date, daily_revenue, AVG(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) AS ma_7day, AVG(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW ) AS ma_30day, STDDEV(daily_revenue) OVER ( ORDER BY order_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW ) AS stddev_30day FROM daily_metrics ) SELECT order_date, daily_revenue, ma_7day, ma_30day, -- Z-score for anomaly detection (daily_revenue - ma_30day) / NULLIF(stddev_30day, 0) AS z_score, -- Day of week patterns day_of_week(order_date) AS day_of_week, -- Month of year seasonality month(order_date) AS month_num FROM moving_averages ORDER BY order_date DESC; Query Performance Optimization:\nPartitioning strategy: Reduced scan by 85% Before: 2.4 TB scanned per query After: 360 GB scanned per query Columnar format (Parquet): 70% compression ratio Partition projection: Eliminated Glue Catalog calls CTAS (Create Table As Select): Materialized frequent queries Bucketing: Optimized join performance Query result reuse: 60-minute caching Average query cost: $0.18 (down from $1.20) Athena Federated Queries:\nConnected data sources: RDS PostgreSQL (operational database) DynamoDB (user sessions) DocumentDB (product catalog) On-premises MySQL (via Lambda connector) Cross-source analytics enabled Query example: Join S3 data lake with live RDS data 5. Enterprise BI with Amazon QuickSight QuickSight Configuration:\nEdition: Enterprise (SAML SSO, row-level security, hourly refresh) Users: 45 authors, 150 readers SPICE capacity: 50 GB allocated Data sources connected: Amazon Athena (primary) Amazon S3 (direct query) Amazon RDS PostgreSQL Salesforce (via connector) Google Analytics (via connector) Dashboard Implementations:\nDashboard 1 - Executive KPI Dashboard:\nMetrics displayed: Total Revenue (current vs previous period) Active Customers (MoM growth %) Average Order Value (trend) Customer Lifetime Value Churn Rate Net Promoter Score (NPS) Visualizations: KPI cards with comparison indicators Line charts for trends Gauge charts for goal tracking Heat map for regional performance Funnel chart for conversion analysis Interactivity: Date range filters Region/Category drill-downs Export to PDF/Excel Email subscriptions (daily/weekly) Auto-refresh: Every 1 hour Users: C-level executives (15 users) Dashboard 2 - Sales Analytics Dashboard:\nAnalysis sections: Sales by product category (bar chart) Geographic sales distribution (map) Top 10 products by revenue (table with sparklines) Sales vs target (combo chart) Customer segmentation (tree map) Month-over-month comparison (line chart) Calculated fields: Revenue Growth % = (sum({revenue}) - sumOver(sum({revenue}), [{period} ASC], 1, 1)) / sumOver(sum({revenue}), [{period} ASC], 1, 1) * 100 YTD Revenue = sumOver(sum({revenue}), [{year}], [{year} = ${current_year}], PRE_AGG) Customer Segment = ifelse({lifetime_value} \u0026gt; 10000, \u0026#34;VIP\u0026#34;, ifelse({lifetime_value} \u0026gt; 5000, \u0026#34;High Value\u0026#34;, ifelse({lifetime_value} \u0026gt; 1000, \u0026#34;Medium Value\u0026#34;, \u0026#34;Low Value\u0026#34;))) Parameters: Date range selector Product category filter Region multi-select Minimum order value threshold Row-Level Security: Sales managers: See only their region Product managers: See only their category Analysts: See all data Users: Sales team (35 users) Dashboard 3 - Real-Time Operations Dashboard:\nLive metrics (1-minute refresh): Current active users Transactions per minute System error rate API response times Inventory alerts Data source: Kinesis Data Streams → Athena live query Alerts configured: Error rate \u0026gt; 2% → Email + Slack notification Response time \u0026gt; 2s → PagerDuty alert Inventory below threshold → Email to ops team Users: Operations team (12 users) QuickSight ML Insights:\nAnomaly detection: Automatically detect unusual patterns Detected 3 anomalies this week: Unexpected revenue spike (holiday promotion - expected) Low traffic on Tuesday (system maintenance - known) Product category shift (new product launch - explained) Forecasting: Revenue forecast for next 30 days Predicted revenue: $1.2M ± $150K (confidence interval) Accuracy on last month: 92% Contribution analysis: Identify key drivers Top revenue drivers: Product category (45%), Region (30%), Customer segment (15%) Embedded Analytics:\nQuickSight dashboards embedded in internal web application SSO integration with corporate IdP Custom branding and white-labeling API-based dashboard provisioning Row-level security enforced Usage: 2,500 dashboard views/week 6. Streaming Analytics Implementation Kinesis Data Streams Configuration:\nStream: clickstream-events Shards: 4 shards (4 MB/sec write, 8 MB/sec read capacity) Retention: 7 days Encryption: KMS encryption enabled Data producers: Web application (user clicks, page views) Mobile app (app events, user actions) IoT devices (sensor readings) Throughput: 3.2 MB/sec average, 6.5 MB/sec peak Records/sec: 850 average, 1,800 peak Kinesis Data Firehose Delivery:\nDelivery streams created: S3 Delivery Stream:\nDestination: s3://datalake-bronze-prod/clickstream/ Buffer size: 5 MB Buffer interval: 300 seconds Compression: Gzip Format conversion: JSON to Parquet Partitioning: year/month/day/hour Data transformation: Lambda function for enrichment OpenSearch Delivery Stream:\nDestination: OpenSearch domain analytics-cluster Index: clickstream-{yyyy-MM-dd} Buffer size: 5 MB Buffer interval: 60 seconds Retry duration: 3600 seconds Use case: Real-time dashboards and search Real-Time Processing with Lambda:\nimport json import base64 from datetime import datetime def lambda_handler(event, context): output = [] for record in event[\u0026#39;records\u0026#39;]: # Decode Kinesis data payload = base64.b64decode(record[\u0026#39;data\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) data = json.loads(payload) # Enrich data data[\u0026#39;processed_timestamp\u0026#39;] = datetime.utcnow().isoformat() data[\u0026#39;partition_key\u0026#39;] = f\u0026#34;{data[\u0026#39;user_id\u0026#39;]}_{data[\u0026#39;session_id\u0026#39;]}\u0026#34; # Classify user action if \u0026#39;purchase\u0026#39; in data.get(\u0026#39;event_type\u0026#39;, \u0026#39;\u0026#39;): data[\u0026#39;event_category\u0026#39;] = \u0026#39;conversion\u0026#39; elif \u0026#39;add_to_cart\u0026#39; in data.get(\u0026#39;event_type\u0026#39;, \u0026#39;\u0026#39;): data[\u0026#39;event_category\u0026#39;] = \u0026#39;engagement\u0026#39; else: data[\u0026#39;event_category\u0026#39;] = \u0026#39;browsing\u0026#39; # Calculate session duration if data.get(\u0026#39;event_type\u0026#39;) == \u0026#39;session_end\u0026#39;: session_duration = data.get(\u0026#39;timestamp\u0026#39;) - data.get(\u0026#39;session_start_time\u0026#39;) data[\u0026#39;session_duration_sec\u0026#39;] = session_duration # Encode back to base64 output_record = { \u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;Ok\u0026#39;, \u0026#39;data\u0026#39;: base64.b64encode(json.dumps(data).encode(\u0026#39;utf-8\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;) } output.append(output_record) return {\u0026#39;records\u0026#39;: output} Kinesis Data Analytics Application:\nApplication type: SQL-based analytics Use case: Real-time aggregations and windowing SQL query example: CREATE OR REPLACE STREAM \u0026#34;DESTINATION_SQL_STREAM\u0026#34; ( window_start TIMESTAMP, event_type VARCHAR(50), event_count INTEGER, unique_users INTEGER, avg_session_duration DOUBLE ); CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP\u0026#34; AS INSERT INTO \u0026#34;DESTINATION_SQL_STREAM\u0026#34; SELECT STREAM STEP(\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME BY INTERVAL \u0026#39;1\u0026#39; MINUTE) AS window_start, \u0026#34;event_type\u0026#34;, COUNT(*) AS event_count, COUNT(DISTINCT \u0026#34;user_id\u0026#34;) AS unique_users, AVG(\u0026#34;session_duration_sec\u0026#34;) AS avg_session_duration FROM \u0026#34;SOURCE_SQL_STREAM_001\u0026#34; GROUP BY STEP(\u0026#34;SOURCE_SQL_STREAM_001\u0026#34;.ROWTIME BY INTERVAL \u0026#39;1\u0026#39; MINUTE), \u0026#34;event_type\u0026#34;; Output: Send aggregated data to: Kinesis Data Streams (for further processing) Lambda (for alerting) OpenSearch (for real-time dashboards) Streaming Analytics Metrics:\nEvents processed: 2.4M events/day Average latency: 1.2 seconds (ingestion to dashboard) Processing cost: $85/day ($2,550/month) Data volume: 3.8 GB/day compressed 7. Comprehensive Monitoring and Operations CloudWatch Dashboards:\nData Pipeline Health Dashboard:\nGlue job success/failure rates ETL job durations Data quality check pass rates S3 bucket storage growth Athena query metrics Kinesis stream throughput Cost Monitoring Dashboard:\nS3 storage costs by layer Glue job execution costs Athena query costs by workgroup Kinesis streaming costs QuickSight user costs Daily cost breakdown CloudWatch Alarms:\nGlue job failures → SNS → Email + Slack Athena cost threshold exceeded → Budget alert Kinesis stream throttling → Auto-scaling trigger Data freshness SLA breach → PagerDuty S3 storage \u0026gt; 10 TB → Cost review notification Data Lineage and Metadata:\nAWS Glue Data Catalog: Source of truth for schemas Table metadata: 347 tables documented Column-level metadata: Data types, descriptions, PII tags Lineage tracking: Source → Bronze → Silver → Gold Impact analysis: Which dashboards affected by schema changes 8. Week 9 Summary and Key Metrics Data Platform Metrics:\nMetric Value Total data ingested 42 GB/day Data sources 12 sources Tables in catalog 347 tables Glue jobs 18 jobs Athena queries/day 450 queries QuickSight dashboards 15 dashboards QuickSight users 195 users Dashboard views/week 2,500 views Data freshness (avg) 12 minutes Query performance (p95) 8 seconds Monthly data platform cost $3,200 Cost per GB processed $0.07 Business Value Delivered:\nReduced time-to-insight from 2 days to 15 minutes (99% improvement) Self-service analytics: 85% of reports now self-generated by business users Data democratization: 195 users with data access (up from 12 analysts) Cost savings: $8,500/month vs traditional DW solution Decision speed: Real-time dashboards enable faster business decisions Technical Achievements:\nImplemented modern medallion architecture 100% serverless data platform (no infrastructure management) Scalable from GB to PB (tested up to 500 GB/day) Comprehensive data governance with Lake Formation Real-time and batch analytics coexistence Advanced ML insights integrated Lessons Learned:\nPartitioning strategy is critical for query performance and cost Parquet format significantly reduces storage and query costs Data quality checks should be implemented early in the pipeline Incremental processing is more efficient than full reprocessing SPICE in QuickSight provides fast dashboard performance Row-level security essential for data governance Athena workgroups enable cost control and usage tracking Streaming and batch architectures complement each other Next Steps:\nImplement ML models with SageMaker using Gold layer data Add more data sources (social media, marketing platforms) Implement data quality scorecards Expand QuickSight embedded analytics Deploy DataZone for data marketplace Implement automated data cataloging with AI "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nUnderstanding VPC Endpoints VPC Endpoints enable private connectivity between your Virtual Private Cloud (VPC) and AWS services without requiring traffic to traverse the public internet. These endpoints are:\nHighly Available: Built with redundancy across multiple Availability Zones Scalable: Automatically scale to handle your workload demands Secure: Keep traffic within the AWS network, reducing exposure to internet-based threats There are two primary types of VPC endpoints:\nGateway Endpoints: Used for connecting to Amazon S3 and DynamoDB. These are specified as route table targets for traffic destined to supported AWS services. Interface Endpoints (AWS PrivateLink): Enable private connectivity to services powered by AWS PrivateLink, including many AWS services, your own services, and SaaS applications. These create elastic network interfaces (ENIs) in your subnets. Lab Architecture Overview This hands-on workshop demonstrates secure access to Amazon S3 from multiple network environments using VPC endpoints. The lab architecture consists of two distinct Virtual Private Clouds:\nCloud VPC Environment:\nHosts cloud-native AWS resources including EC2 instances for testing Contains a Gateway VPC Endpoint providing direct, private access to Amazon S3 Represents your production AWS cloud infrastructure Demonstrates how cloud workloads can securely access S3 without internet exposure On-Premises Simulation VPC:\nEmulates a traditional on-premises data center or branch office environment Includes an EC2 instance configured with VPN software (OpenSwan/strongSwan) to establish secure connectivity Connected to AWS through a Site-to-Site VPN via AWS Transit Gateway Features an Interface VPC Endpoint enabling on-premises resources to access S3 privately over the VPN connection Simulates hybrid cloud scenarios where on-premises applications need secure S3 access Network Connectivity: The two VPCs are interconnected through AWS Transit Gateway, which acts as a cloud router to enable secure communication between your cloud and simulated on-premises environments. The Site-to-Site VPN connection ensures encrypted traffic flow between the environments.\nImportant Notes:\nThis lab uses a single VPN tunnel for cost efficiency and simplicity For production deployments, AWS strongly recommends implementing redundant VPN connections across multiple devices and Availability Zones for high availability The architecture follows AWS Well-Architected Framework principles for security and reliability This workshop will guide you through configuring both Gateway and Interface VPC Endpoints, testing connectivity from both environments, and understanding best practices for securing S3 access in hybrid cloud architectures.\n"},{"uri":"https://workshop-sample.fcjuni.com/2-proposal/","title":"Proposal","tags":[],"description":"","content":" Note: The information below is for reference purposes only, please do not copy verbatim for your report including this warning.\nDevteria Game Store Platform AWS Cloud E-commerce Solution for Digital Game Distribution 📄 Project Proposal Document Devteria Shop Game Project With AWS (Word)\n1. Executive Summary Devteria Game Store is a scalable e-commerce platform for digital game licensing. Built on AWS, it delivers secure authentication, real-time inventory, automated order processing, and global content delivery. Supports thousands of concurrent users with 99.9% uptime and cost efficiency through serverless architecture.\n2. Problem Statement Current Challenges:\nTraditional stores struggle with traffic spikes Complex auth/payment reduces conversions Manual inventory causes overselling Lack of real-time analytics High infrastructure costs for peak capacity Solution: Devteria leverages AWS: CloudFront + S3 (fast delivery), Cognito (secure auth), API Gateway + Lambda (serverless backend), RDS + S3 (reliable storage), SQS + SNS (async processing), CodePipeline (CI/CD).\n3. Solution Architecture Core Components:\nFrontend: CloudFront CDN + S3 (React app, global cache, \u0026lt;2s load) Backend: API Gateway + Lambda (auto-scaling logic) + ALB + EC2 (microservices) Data: RDS PostgreSQL (users, catalog, orders) + S3 (game files, assets) + SQS/SNS (async processing) Security: Cognito (auth with MFA) + IAM (access control) + CloudWatch (monitoring) CI/CD: GitLab CodePipeline CodeBuild Deploy User Flow: Access site Login (Cognito) Browse games (API/Lambda/RDS) Add to cart Checkout License generation (SQS) Email (SNS) Secure download (S3)\n4. AWS Services Service Purpose Configuration CloudFront CDN 10M requests, 50GB transfer S3 Storage 100GB (frontend + assets) API Gateway API Management 1M requests/month Lambda Serverless Compute 5M invocations, 512MB EC2 Microservices 2x t3.medium RDS Database db.t3.small Multi-AZ ALB Load Balancer 1 ALB Cognito Authentication 10K users SQS + SNS Queue + Notifications 5M + 100K messages CloudWatch Monitoring Metrics + logs CodePipeline CI/CD 1 pipeline 5. Implementation Timeline (6 months) Month Milestones 1 Infrastructure: Setup AWS, VPC, RDS, S3, Cognito 2-3 Backend: Lambda APIs (auth, catalog, orders) + API Gateway 3 Frontend: React/Next.js app + Cognito integration 4 Advanced: Payment gateway + Admin dashboard + CI/CD 5 Testing: Load tests + Security audits + Performance tuning 6 Launch: Beta release Public launch 6. Budget Estimate Monthly Cost (10K users, 1K orders/month): ~$228\nService Cost CloudFront + S3 + API Gateway + Lambda $32 EC2 (2x t3.medium) + RDS (t3.small) $110 ALB + Cognito $50 SQS + SNS + CloudWatch + Other $36 Scaling: 50K users ($650/month), 100K users ($1,200/month)\nOne-time: Development ($5K-8K), Domain ($15/year), SSL (Free via ACM)\n7. Risk Assessment Risk Mitigation DDoS attacks AWS Shield, CloudFront, rate limiting Data breaches Encryption, IAM, regular audits Payment fraud 3D Secure, fraud detection Lambda cold starts Provisioned concurrency Cost overruns Budget alerts, auto-scaling limits Contingency: RDS automated backups, Multi-AZ deployment, CodePipeline rollback, static maintenance page\n8. Expected Outcomes Technical:\nPerformance: \u0026lt;2s page load globally Scalability: Handle 10x traffic spikes Reliability: 99.9% uptime Security: Zero breaches, PCI-ready Business:\n40% reduction in cart abandonment 60% less infrastructure management time 25-35% revenue increase from better UX Global market reach via CDN Long-term: Scale to 100K+ users, team gains AWS expertise, reusable microservices, rapid feature development\nNext Steps Proposal approval AWS account setup Team assembly Start Phase 1 Weekly progress reviews "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.2-event2/","title":"Event 2: Vietnam Cloud Day 2025 - Track for Builders","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nEvent Report “Vietnam Cloud Day 2025: Track for Builders” Time and Theme Time: Thursday, September 18, 2025 (1:15 PM – 5:00 PM) Theme: Track B: Gen AI and Data Event Objectives Share strategies and best practices for building a unified, scalable Data Foundation on AWS. Introduce the strategic roadmap and adoption trends for Generative AI (GenAI) in organizations. Analyze the role of AI in the Development Lifecycle (AI-DLC) to improve software speed and quality. Guide methods and services for securing GenAI applications at every layer (Infrastructure, Models, Applications). List of Speakers Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Highlights of the Content 1. Building a Unified Data Foundation on AWS for AI and Analytics Workloads Unified Data Foundation: Strategies for building a flexible, scalable data infrastructure to meet the demands of modern AI and Analytics applications. Core Components: Emphasis on Data Ingestion, Storage, Processing, and Data Governance aspects on AWS. 2. Building the Future: Gen AI Adoption and Roadmap on AWS Vision and Trends: Sharing a comprehensive vision, key trends, and AWS\u0026rsquo;s strategic roadmap for Generative AI adoption. GenAI Services: Introduction of AWS services and initiatives that help organizations leverage GenAI to drive innovation and efficiency. 3. AI-Driven Development Lifecycle (AI-DLC) Shaping the future of Software Implementation Transforming SDLC: Reshaping the software development process by embedding AI as a central collaborator throughout the entire lifecycle. AI-powered Execution: Integrating AI into the execution process with human oversight to significantly improve speed, quality, and innovation capability. 4. Securing Generative AI Applications with AWS: Fundamentals and Best Practices GenAI Security Challenges: Exploring unique security risks at each layer of the GenAI stack (Infrastructure, Models, Applications). Best Practices: Guidance on how AWS integrates security measures like encryption, Zero-Trust architecture, continuous monitoring, and granular access controls to protect GenAI workloads. 5. Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers AI Agents: Discussion on the paradigm shift from basic automation to utilizing AI Agents (intelligent partners) capable of self-learning, adaptation, and autonomous execution of complex tasks. Key Takeaways Data \u0026amp; AI Mindset Data Foundation is Key: All AI/Analytics efforts must start with a unified, well-governed data foundation (Data Governance) to ensure data quality and security. GenAI is Strategy, not just a Tool: A clear roadmap is needed to integrate GenAI into core business and operational processes. Technical Architecture \u0026amp; Security Multi-layered Security in GenAI: Security is not limited to infrastructure; it must protect the Model and the Application Flow using measures like encryption and Zero-Trust. AI-DLC Replaces Traditional SDLC: Learned how to embed AI into the software development lifecycle to accelerate speed and innovation, treating AI as a collaborator. Application to Work Strengthen Data Governance: Apply the data governance methods learned to establish consistent data storage, processing, and security principles for future AI/ML projects. Experiment with AI-DLC: Research and pilot the integration of AI tools (like Amazon Q) into the development workflow to automate repetitive tasks. Secure Design for GenAI: When designing GenAI applications, ensure that multi-layered security principles (data encryption, access control) are applied from the start. Event Experience Attending the Track for Builders was a highly valuable experience, marking a shift in focus from traditional infrastructure to data-driven and AI architecture.\nFoundational and Strategic Knowledge: I gained access not just to specific AWS services but also the overall strategy on how to build a data foundation for AI (Kien Nguyen) and the GenAI roadmap across the entire organization (Jun Kai Loke, Tamelly Lim). Learning Modern Security: Taiki Dang\u0026rsquo;s presentation on GenAI security was highly useful, clarifying new vulnerabilities in the GenAI stack and how traditional security measures are extended (e.g., Zero-Trust). Vision for Software Development: Binh Tran\u0026rsquo;s talk on AI-DLC changed my perspective on SDLC, placing AI at the center of software building. Overall, this event equipped me with the mindset and tools necessary to not only use GenAI but also to strategically build and secure GenAI applications safely.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Creating Interface VPC Endpoint for S3 Access","tags":[],"description":"","content":" ⚠️ Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will deploy an S3 Interface VPC Endpoint within the cloud VPC. This endpoint enables private connectivity from your simulated on-premises network to Amazon S3, eliminating the need for internet gateways or NAT devices.\nWhy Interface Endpoints for Hybrid Access?\nInterface VPC Endpoints create elastic network interfaces (ENIs) with private IP addresses in your VPC. This architecture provides several advantages for hybrid cloud scenarios:\nPrivate IP Addressing: On-premises systems can resolve S3 service names to private IPs within your VPC CIDR range DNS Integration: Works seamlessly with Route 53 Resolver for hybrid DNS resolution Security: Traffic flows through private connections (VPN/Direct Connect) without traversing the public internet PrivateLink Technology: Leverages AWS PrivateLink for secure, scalable service access Fine-Grained Access Control: Apply security groups and endpoint policies for granular access management Interface vs Gateway Endpoints: While you previously used a Gateway Endpoint (which modifies route tables), Interface Endpoints use ENIs with private IPs. This makes them suitable for on-premises access through VPN/Direct Connect, as they can be reached via standard IP routing.\nStep-by-Step Interface Endpoint Creation Step 1: Navigate to VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation menu, click Endpoints Click the Create endpoint button in the top-right corner Step 2: Configure Endpoint Basic Settings\nIn the Create Endpoint configuration page:\nName your endpoint (optional but recommended):\nEnter a descriptive name: S3-Interface-Endpoint-HybridAccess This helps identify the endpoint\u0026rsquo;s purpose in production environments Select Service Category:\nChoose AWS services This filters the service list to AWS-managed endpoints Naming Convention Best Practice: Use descriptive names that indicate the service, endpoint type, and purpose, e.g., S3-Interface-CloudVPC or S3-InterfaceEP-Production.\nStep 3: Locate and Select the S3 Interface Service\nSearch for S3 service:\nIn the Services search box, type: S3 Press Enter or click the search icon Identify the correct service:\nLook for the service named: com.amazonaws.us-east-1.s3 Verify the Type column shows \u0026ldquo;Interface\u0026rdquo; (not Gateway) Click the radio button to select this service Critical Selection: AWS offers both Gateway and Interface endpoint types for S3. Ensure you select the Interface type. Gateway endpoints won\u0026rsquo;t work for on-premises access scenarios because they only modify VPC route tables.\nStep 4: Configure VPC and DNS Settings\nSelect Target VPC: From the VPC dropdown, select VPC Cloud Do NOT select \u0026ldquo;VPC On-prem\u0026rdquo; - the endpoint must be in the cloud VPC VPC Selection is Critical: The Interface Endpoint must be created in VPC Cloud, not VPC On-prem. On-premises systems will access this endpoint through the VPN tunnel you configured earlier.\nConfigure DNS Settings: Expand the Additional settings section Uncheck \u0026ldquo;Enable DNS name\u0026rdquo; (disable it) Why? You\u0026rsquo;ll manually configure DNS using Route 53 Private Hosted Zones for more control DNS Configuration Choice: In this workshop, you\u0026rsquo;re using Route 53 Resolver with Private Hosted Zones for DNS resolution (configured in section 5.4.1). This provides greater flexibility and mirrors enterprise hybrid DNS architectures. Alternatively, enabling \u0026ldquo;Enable DNS name\u0026rdquo; would automatically create Route 53 private DNS records, but we want explicit control for learning purposes.\nStep 5: Select Availability Zones and Subnets\nConfigure High Availability:\nSelect 2 subnets in different Availability Zones: Availability Zone: us-east-1a Choose the corresponding private subnet in VPC Cloud Availability Zone: us-east-1b Choose the corresponding private subnet in VPC Cloud Why Multiple AZs?\nDeploying Interface Endpoint ENIs in multiple Availability Zones provides:\nHigh Availability: If one AZ experiences issues, traffic can route through the other Fault Tolerance: Automatic failover between ENIs in different AZs Geographic Redundancy: Reduces latency by having endpoints in multiple locations Production Best Practice: AWS recommends multi-AZ deployment for critical services Best Practice: Always deploy Interface Endpoints across at least 2 Availability Zones for production workloads. This ensures your on-premises applications maintain S3 connectivity even during AZ failures.\nStep 6: Apply Security Group\nSelect Security Group: In the Security groups dropdown, choose SGforS3Endpoint This security group was created by the CloudFormation template It contains inbound rules allowing HTTPS (port 443) traffic from on-premises CIDR Security Group Purpose:\nThe SGforS3Endpoint security group controls which sources can communicate with the Interface Endpoint ENIs:\nInbound Rules: - Protocol: TCP - Port: 443 (HTTPS) - Source: VPC On-prem CIDR (e.g., 10.1.0.0/16) - Purpose: Allow S3 API calls from on-premises systems Security Layer: Security groups act as virtual firewalls for your Interface Endpoint. Even though traffic comes through a VPN, the security group provides an additional layer of control, following AWS defense-in-depth principles.\nStep 7: Configure Endpoint Policy\nPolicy Configuration:\nLeave the Policy set to Full access (default) This allows all S3 actions through the endpoint Understanding Endpoint Policies:\nEndpoint policies control what AWS API actions can be performed through the endpoint They work in conjunction with IAM policies (both must allow an action) For this workshop, full access simplifies testing Click Create endpoint\nProduction Consideration: In production environments, use restrictive endpoint policies following the principle of least privilege. For example, limit to specific S3 buckets or actions like s3:GetObject and s3:PutObject only.\nExample Restrictive Policy:\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-company-data/*\u0026#34;, \u0026#34;arn:aws:s3:::my-company-data\u0026#34; ] } ] } Verify Endpoint Creation Check Endpoint Status:\nThe endpoint creation typically takes 2-3 minutes Refresh the Endpoints page periodically Wait until the Status changes from Pending to Available Once available, note the Endpoint ID (format: vpce-xxxxxxxxxxxxxxxxx) Inspect Endpoint Details:\nClick on the newly created endpoint to view:\nDNS names: Private DNS names assigned to the endpoint Subnets: Verify both AZs are listed with their ENI IDs Network interfaces: Each subnet has a dedicated ENI with a private IP Security groups: Confirm SGforS3Endpoint is attached Record Private IP Addresses:\nIn the Subnets section, note the private IP addresses assigned to each ENI. You\u0026rsquo;ll use these in DNS configuration (next step).\nUnderstanding What You Created Architecture Overview:\nYou\u0026rsquo;ve deployed an Interface VPC Endpoint with the following components:\nTwo Elastic Network Interfaces (ENIs):\nOne ENI in us-east-1a subnet with a private IP (e.g., 10.0.1.100) One ENI in us-east-1b subnet with a private IP (e.g., 10.0.2.100) Security Group Protection:\nENIs protected by SGforS3Endpoint security group Only allows HTTPS traffic from on-premises CIDR Private Connectivity Path:\nOn-Premises Systems → VPN Tunnel → VPC Cloud → Interface Endpoint ENI → AWS PrivateLink → S3 Service How On-Premises Access Works:\nWhen an on-premises application makes an S3 API call:\nDNS query for s3.amazonaws.com is sent to Outbound Resolver Outbound Resolver forwards through VPN to Inbound Resolver Inbound Resolver queries Private Hosted Zone (to be configured next) Private Hosted Zone returns Interface Endpoint private IP Application sends HTTPS request to private IP through VPN Request reaches Interface Endpoint ENI PrivateLink routes request to S3 service Response follows reverse path back to on-premises Congratulations! You\u0026rsquo;ve successfully created an S3 Interface VPC Endpoint for hybrid cloud access. This endpoint provides a secure, private connection from your on-premises environment to Amazon S3, eliminating internet exposure and enabling enterprise-grade hybrid architectures.\nNext Steps With the Interface Endpoint created, you\u0026rsquo;ll proceed to:\nConfigure DNS records in Route 53 Private Hosted Zone to resolve S3 DNS names to endpoint IPs Test connectivity from the on-premises EC2 instance to S3 through the private endpoint Verify traffic flow using VPC Flow Logs and endpoint metrics Validate security by confirming all traffic stays within AWS private networks The Interface Endpoint is now ready to serve as your private gateway to S3 for hybrid workloads!\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nRequired IAM Permissions Before starting this workshop, you must ensure your AWS IAM user or role has sufficient permissions to create and manage the necessary resources. Attach the following custom IAM policy to your user account or assume a role with these permissions.\nImportant Security Notes:\nReview all permissions before applying to understand what resources will be created These permissions are required for workshop deployment and cleanup operations After completing the workshop, consider removing these permissions if they\u0026rsquo;re no longer needed For production environments, always follow the principle of least privilege { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Policy Coverage: This IAM policy grants permissions across multiple AWS services essential for the workshop:\nEC2 \u0026amp; VPC Networking: Create and manage VPCs, subnets, route tables, security groups, Transit Gateway, VPN connections, and VPC endpoints CloudFormation: Deploy and manage infrastructure as code stacks IAM: Create roles and instance profiles for EC2 instances S3: Create buckets and manage objects for workshop resources Lambda \u0026amp; CloudWatch: Deploy serverless functions and monitor resources Route53: Configure DNS and resolver endpoints SSM (Systems Manager): Enable secure instance access via Session Manager Secrets Manager: Store and retrieve sensitive configuration data Environment Setup Using Infrastructure as Code Workshop Region: This lab is designed for the US East (N. Virginia) region (us-east-1). Ensure you select this region before proceeding.\nAutomated Deployment Overview: Instead of manually creating each resource, we\u0026rsquo;ll use AWS CloudFormation to automate the entire infrastructure setup. This Infrastructure as Code (IaC) approach provides:\nConsistency: All participants work with identical configurations Speed: Deploy complex multi-resource environments in minutes Repeatability: Easy to recreate or clean up the environment Best Practices: Pre-configured with AWS recommended settings Deployment Steps:\nLaunch the CloudFormation Stack:\nClick this link to open the CloudFormation quick-create console: Deploy Workshop Infrastructure The template URL is pre-populated with the workshop configuration Stack name is automatically set to PLCloudSetup Review Stack Parameters: Review the pre-configured parameters (all defaults are optimized for this workshop):\nAcknowledge IAM Resource Creation: Scroll to the bottom of the page Check both acknowledgment boxes: ✓ \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources.\u0026rdquo; ✓ \u0026ldquo;I acknowledge that AWS CloudFormation might create IAM resources with custom names.\u0026rdquo; Click Create stack Monitor Deployment Progress: CloudFormation will begin provisioning resources Expected deployment time: approximately 15-20 minutes You can monitor progress in the CloudFormation console under the \u0026ldquo;Events\u0026rdquo; tab The stack uses nested stacks to organize resource creation logically What Gets Created: The CloudFormation template automatically provisions:\n2 Virtual Private Clouds (VPCs): One simulating cloud environment, one simulating on-premises Multiple Subnets: Public and private subnets across availability zones AWS Transit Gateway: Central hub for VPC connectivity Site-to-Site VPN: Pre-configured VPN connection between VPCs 3 EC2 Instances: Test instances in different network segments Security Groups: Pre-configured for workshop traffic IAM Roles: Instance profiles for EC2 instances Route Tables: Properly configured routing between environments Verify Deployment:\nOnce the stack status shows CREATE_COMPLETE, verify the resources:\nCheck VPCs Created: Navigate to VPC Dashboard and confirm two VPCs are present: Cloud VPC (for AWS cloud resources) On-Premises VPC (simulating datacenter) Check EC2 Instances: Navigate to EC2 Dashboard and verify three instances are running: Cloud Test Instance (in Cloud VPC) On-Prem Test Instance (in On-Prem VPC) VPN Gateway Instance (for Site-to-Site connectivity) Troubleshooting:\nIf stack creation fails, check the \u0026ldquo;Events\u0026rdquo; tab for error messages Ensure your IAM user has all required permissions from the policy above Verify you\u0026rsquo;re in the us-east-1 region Check AWS service quotas for EC2, VPC, and CloudFormation if you encounter limits You\u0026rsquo;re now ready to proceed with the workshop exercises!\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Testing Gateway Endpoint Connectivity","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nNow that you\u0026rsquo;ve created the Gateway Endpoint, it\u0026rsquo;s time to verify that it works correctly. In this section, you\u0026rsquo;ll create an S3 bucket, connect to an EC2 instance in your VPC, and upload a test file to S3 - all without traffic leaving the AWS network.\nTesting Objectives:\nVerify private connectivity to S3 through the Gateway Endpoint Confirm that traffic does not traverse the public internet Demonstrate successful S3 operations from a private subnet Validate route table configuration Step 1: Create an S3 Bucket for Testing First, you need an S3 bucket to test uploads through the Gateway Endpoint.\nNavigate to S3 Console: Open the S3 Management Console Click the Create bucket button Configure Bucket Settings:\nBucket Name:\nEnter a globally unique name (S3 bucket names must be unique across all AWS accounts) Suggested format: gateway-endpoint-test-\u0026lt;your-initials\u0026gt;-\u0026lt;random-number\u0026gt; Example: gateway-endpoint-test-jd-20241207 AWS Region:\nEnsure the region is US East (N. Virginia) us-east-1 This must match the region where your VPC and Gateway Endpoint are deployed Object Ownership:\nLeave as default (ACLs disabled - Recommended) Block Public Access settings:\nLeave all options checked (default) This ensures the bucket remains private Bucket Versioning:\nLeave as default (Disabled) Default Encryption:\nLeave as default (Server-side encryption with Amazon S3 managed keys - SSE-S3) Create the Bucket: Scroll to the bottom Click Create bucket Verify Creation: You should see a success message Your new bucket appears in the buckets list Security Best Practice: Notice that the bucket is created with all public access blocked by default. This aligns with AWS security best practices. Access will only be possible through the VPC endpoint and with proper IAM permissions.\nStep 2: Connect to EC2 Instance Using Session Manager You\u0026rsquo;ll use AWS Systems Manager Session Manager to securely connect to the EC2 instance without requiring SSH keys, bastion hosts, or open inbound ports.\nWhat is AWS Session Manager? Session Manager is a fully managed Systems Manager capability that provides:\nSecure Access: Browser-based shell access without opening inbound ports Audit Trail: All sessions are logged in CloudWatch Logs No SSH Keys: No need to manage or distribute SSH keys Cross-Platform: Works with Linux and Windows instances Architecture Context: The EC2 instance you\u0026rsquo;ll connect to is running in a private subnet within \u0026ldquo;VPC Cloud\u0026rdquo;. It has no direct internet access - connectivity to AWS services (including Systems Manager) is provided through the SSM VPC Interface Endpoints that were created during CloudFormation deployment.\nOpen Systems Manager Console: In the AWS Management Console search bar, type Systems Manager Press Enter or click on the Systems Manager service Navigate to Session Manager: In the left navigation pane, locate Node Management section Click on Session Manager Start a New Session: Click the Start session button From the list of available instances, select the instance named Test-Gateway-Endpoint Instance Details: This EC2 instance is deployed in a private subnet of \u0026ldquo;VPC Cloud\u0026rdquo; and is configured with an IAM instance profile that grants necessary permissions for Session Manager and S3 access. The instance will be used to verify that S3 traffic flows through the Gateway Endpoint you created, rather than the public internet.\nSession Established: Session Manager opens a new browser tab with a shell prompt You should see a prompt like: sh-4.2$ or similar This indicates you\u0026rsquo;re successfully connected to the EC2 instance Connectivity Verification: The fact that Session Manager works confirms that the SSM Interface Endpoints are functioning correctly. If you couldn\u0026rsquo;t connect, it would indicate an issue with the Interface Endpoints or IAM permissions.\nStep 3: Create and Upload Test File to S3 Now you\u0026rsquo;ll create a test file on the EC2 instance and upload it to S3 through the Gateway Endpoint.\nNavigate to Home Directory:\nExecute the following command to switch to the ssm-user\u0026rsquo;s home directory:\ncd ~ Why this step? Working in the home directory ensures you have proper write permissions and a clean working environment.\nCreate a Test File:\nGenerate a 1GB test file using the following command:\nfallocate -l 1G testfile.xyz Command Explanation:\nfallocate: Efficiently allocates space for a file -l 1G: Specifies file size of 1 gigabyte testfile.xyz: The filename for our test file This creates a 1GB file almost instantly by allocating disk space without writing actual data. We use a large file to make the upload more observable and to demonstrate that the Gateway Endpoint can handle substantial data transfers.\nUpload File to S3 Bucket:\nUse the AWS CLI to upload the file to your S3 bucket:\naws s3 cp testfile.xyz s3://YOUR-BUCKET-NAME/ Important: Replace YOUR-BUCKET-NAME with the actual name of the bucket you created in Step 1.\nWhat\u0026rsquo;s Happening Behind the Scenes:\nThe EC2 instance makes an API call to S3 The VPC route table directs S3 traffic to the Gateway Endpoint (prefix list destination) Traffic flows through the Gateway Endpoint to S3, staying within the AWS network No internet gateway or NAT gateway is used Success Indicator: If the upload completes successfully, you\u0026rsquo;ll see output showing the file transfer progress and completion. This confirms that:\nThe Gateway Endpoint is configured correctly Route table entries are working as expected The EC2 instance has proper IAM permissions for S3 Private connectivity to S3 is functioning Terminate the Session: Type exit or close the browser tab The session will be cleanly terminated Step 4: Verify Object in S3 Bucket Finally, confirm that the file was successfully uploaded by checking the S3 console.\nReturn to S3 Console:\nNavigate back to the S3 Management Console Open Your Bucket:\nClick on the name of the bucket you created earlier Verify File Presence:\nYou should see testfile.xyz listed in the Objects tab The file size should show approximately 1 GB Note the upload timestamp Verification Checklist:\n✓ File testfile.xyz appears in bucket ✓ File size matches (1 GB) ✓ Upload timestamp is recent ✓ No errors in S3 console Understanding What You Just Accomplished Network Path Verification: In this test, you successfully:\nCreated Private Connectivity: The Gateway Endpoint enabled private connectivity from your VPC to S3 Bypassed Public Internet: The upload did not traverse the public internet - it stayed entirely within AWS\u0026rsquo;s network backbone Used Route-Based Routing: The route table automatically directed S3 traffic to the Gateway Endpoint based on the S3 prefix list Demonstrated Cost Savings: By using a Gateway Endpoint instead of a NAT Gateway, you avoided: NAT Gateway hourly charges NAT Gateway data processing charges Potential internet gateway data transfer costs Traffic Flow Diagram:\nEC2 Instance → VPC Route Table → Gateway Endpoint → S3 Service (Private Subnet) (Routes S3 to vpce-xxx) (No internet) (AWS Network) Key Differences from Internet-Based Access:\nWithout Gateway Endpoint: EC2 → NAT Gateway → Internet Gateway → Public Internet → S3 With Gateway Endpoint: EC2 → Gateway Endpoint → S3 (all within AWS network) Performance Benefits: Gateway Endpoints not only save costs but also typically provide better performance than internet-based access due to:\nLower latency (direct AWS network path) Higher available bandwidth No NAT Gateway bottleneck Dedicated AWS backbone infrastructure Section Summary Congratulations! You\u0026rsquo;ve successfully completed the Gateway Endpoint testing exercise.\nWhat You Learned:\n✓ How to create and configure S3 buckets with security best practices ✓ Using AWS Systems Manager Session Manager for secure instance access ✓ Uploading objects to S3 through a Gateway VPC Endpoint ✓ Verifying private connectivity without internet gateway dependency ✓ Understanding the traffic flow in a VPC with Gateway Endpoints Key Takeaways:\nGateway Endpoints provide cost-effective private connectivity to S3 and DynamoDB No additional charges for using Gateway Endpoints - you only pay for S3 storage and requests Traffic remains private - never exposed to the public internet Automatic routing through route table entries - no complex configuration needed Scalable and highly available - AWS manages the endpoint infrastructure Next Steps: In the following sections, you\u0026rsquo;ll explore:\nCreating Interface VPC Endpoints for other AWS services Implementing VPC Endpoint policies for fine-grained access control Testing connectivity from on-premises (simulated) environments Understanding the differences between Gateway and Interface Endpoints "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Learn about AWS security best practices. Understand monitoring and compliance tools. Practice implementing security controls. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Overview of AI/ML on AWS - Learn about ML support services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey 3 - Practice with Amazon SageMaker: + Create Notebook Instance + Train simple models (Linear Regression / Image Classification) + Deploy endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey 4 - Get familiar with Amazon Rekognition - Demo face and object recognition in images/videos - Integrate Rekognition API into a small web application 12/11/2025 12/11/2025 AWS Journey 5 - Practice Amazon Comprehend (natural language processing) - Experiment with Amazon Kendra (contextual intelligent search) - Compare advantages and limitations of each service 13/11/2025 13/11/2025 AWS Journey 6 - Summarize Week 10 knowledge: + AI/ML model development process on AWS + Real-world applications of AI/ML in business + Write practice results report and expansion directions 14/11/2025 14/11/2025 AWS Journey Week 10 Achievements: Learned about AWS security:\nUnderstood shared responsibility model Reviewed AWS security best practices Studied security pillar of Well-Architected Framework Worked with AWS CloudTrail:\nEnabled CloudTrail in AWS account Configured trail to log to S3 bucket Reviewed API call history Viewed events in CloudTrail console Understood log file format Practiced with AWS Config:\nEnabled AWS Config service Set up Config rules for compliance: Check for encrypted EBS volumes Verify S3 bucket versioning Check IAM password policy Reviewed compliance dashboard Viewed non-compliant resources Used AWS GuardDuty:\nEnabled GuardDuty for threat detection Reviewed finding types Viewed sample findings Understood severity levels (Low, Medium, High) Learned about common threats detected Explored AWS Security Hub:\nEnabled Security Hub Viewed consolidated security findings Reviewed security scores Understood integration with GuardDuty and Config Exported findings report "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Learn about Modernization and Serverless concepts. Understand monolithic vs microservices architectures. Practice building serverless applications with Lambda, API Gateway, and other AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Introduction to Modernization and Serverless concepts - Compare monolithic and microservices architectures - Analyze the benefits of transitioning to serverless 17/11/2025 17/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice AWS Lambda: create functions, configure triggers, view logs in CloudWatch - Deploy basic API processing logic using Lambda 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Integrate API Gateway with Lambda to create REST API - Connect data with DynamoDB (CRUD operations) - Test API using Postman 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Configure Cognito for user authentication (user pool, token) - Integrate Cognito authentication into API Gateway - Manage access permissions via IAM Role 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice deploying a complete Serverless App using AWS SAM (Serverless Application Model) - Testing, logging, and performance optimization - Summarize knowledge and weekly report 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood Serverless Architecture:\nLearned serverless concepts and benefits Compared monolithic vs microservices architectures Studied advantages of serverless: No server management Auto-scaling Pay-per-use pricing Faster time to market Practiced AWS Lambda:\nCreated Lambda functions using Python Configured function triggers (API Gateway, S3, CloudWatch Events) Viewed execution logs in CloudWatch Set environment variables and timeout settings Deployed basic API processing logic Built REST API with API Gateway and Lambda:\nCreated REST API using API Gateway Integrated Lambda functions as backend Implemented CRUD operations with DynamoDB Tested API endpoints using Postman Configured request/response mapping Implemented Authentication with Cognito:\nCreated Cognito User Pool Configured user authentication flow Integrated Cognito with API Gateway Used JWT tokens for authorization Managed IAM roles for access control Deployed Serverless Application:\nLearned AWS SAM framework basics Created SAM template (template.yaml) Deployed complete serverless app using SAM CLI Tested application functionality Reviewed CloudWatch logs for monitoring "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":" ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Review all core AWS services learned during the internship. Design and implement a final project integrating multiple AWS services. Prepare final report and presentation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudWatch, CloudFront, API Gateway - Define requirements and architecture for the final project 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Start project deployment: + Design VPC, subnet, security groups + Configure S3, CloudFront, RDS/DynamoDB (depending on project) 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue project implementation: + Build backend using Lambda / API Gateway or EC2 (depending on architecture) + Connect database and process data + Integrate CloudWatch for monitoring 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Complete the project: + Add Cognito authentication if needed + Finalize CI/CD pipeline (CodePipeline/CodeBuild) + End-to-end system testing 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Write final report - Prepare presentation (architecture, service selection rationale, cost, security) - Summarize the entire learning journey and self-evaluate capabilities 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Reviewed Core AWS Services:\nEC2, S3, RDS, DynamoDB IAM, VPC, Security Groups Lambda, API Gateway CloudWatch, CloudFront Cognito, CodePipeline Planned Final Project:\nDefined project requirements Designed system architecture Selected appropriate AWS services Created architecture diagram Planned implementation steps Implemented Project Infrastructure:\nDesigned VPC with public and private subnets Configured security groups and network ACLs Set up S3 buckets for storage Configured CloudFront for content delivery Created RDS database instance Built Application Backend:\nImplemented Lambda functions for business logic Created API Gateway REST endpoints Connected Lambda to database Configured CloudWatch logs and monitoring Set up error handling and retry logic Completed Project Features:\nIntegrated Cognito for user authentication Set up CodePipeline for CI/CD Performed end-to-end testing Fixed bugs and optimized performance Verified all features working correctly Prepared Final Deliverables:\nWrote comprehensive project report Created presentation slides covering: Architecture overview Service selection rationale Security implementation Cost estimation Summarized 12-week learning journey Self-evaluated technical capabilities gained "},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.3-event3/","title":"Event 3: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nEvent Report “AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS” Time and Theme Time: Saturday, November 15, 2025 (8:30 AM – 12:00 PM) Theme: AI/ML/GenAI on AWS Event Objectives Provide an overview of AWS AI/ML services. Detail Amazon SageMaker – the end-to-end ML platform. Deep dive into Generative AI (GenAI) and Foundation Models via Amazon Bedrock. Guide attendees through Prompt Engineering techniques and the Retrieval-Augmented Generation (RAG) architecture. List of Speakers Detailed speaker information is not available in the original data, assumed to be AWS Vietnam Solution Architects. Highlights of the Content 1. AWS AI/ML Services Overview Amazon SageMaker: Covered the entire Machine Learning (ML) lifecycle, including data preparation, model training, tuning, deployment, and integrated MLOps. Live Demo: Hands-on demonstration with SageMaker Studio. 2. Generative AI with Amazon Bedrock Foundation Models: Comparison and guidance on choosing between models like Claude, Llama, Titan. Prompt Engineering: Advanced techniques such as Chain-of-Thought reasoning and Few-shot learning. Retrieval-Augmented Generation (RAG): Architecture and integration with a Knowledge Base to enhance accuracy and context for GenAI. Bedrock Agents \u0026amp; Guardrails: Building multi-step workflows and Guardrails (Content moderation layers) to ensure safety. Key Takeaways GenAI and ML Skills End-to-end ML Deployment: Mastered the basic steps for building and deploying ML models on SageMaker effectively. Bedrock Proficiency: Clearly understood the components of Bedrock and the capabilities of different Foundation Models. RAG Technique: Grasping the RAG architecture as a critical factor for integrating GenAI into real-world enterprise applications using proprietary data. Application to Work Building GenAI Proof-of-Concepts (PoC): Use Amazon Bedrock to quickly prototype GenAI use cases (e.g., document summarization, internal knowledge-based chatbot) with the RAG technique. Prompt Optimization: Apply the learned Prompt Engineering techniques to improve the quality of responses from large language models. Event Experience In-depth Practice: The Live Demos on SageMaker and Bedrock were highly valuable, providing a practical view of service deployment. Focus on Detail: The event delved into technical aspects like model selection and input optimization (Prompting), giving me a more solid knowledge base compared to purely introductory events. This event provided a strong foundation for me to start working on Machine Learning and Generative AI projects on AWS.\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Testing Interface Endpoint Connectivity from On-Premises","tags":[],"description":"","content":" ⚠️ Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will validate that the Interface VPC Endpoint is working correctly by testing S3 connectivity from the simulated on-premises environment. This demonstrates how enterprise applications running in corporate datacenters can securely access AWS services through private connections.\nTesting Objectives:\nRetrieve the Interface Endpoint DNS name for S3 API calls Connect to an EC2 instance simulating an on-premises server Upload data to S3 using the private endpoint connection Verify successful data transfer through the hybrid architecture Why This Test Matters:\nThis validation confirms that your hybrid networking configuration is functioning correctly:\nDNS resolution from on-premises to endpoint private IPs VPN routing directing traffic to the cloud VPC Interface Endpoint ENIs receiving and processing S3 API requests Security groups allowing proper traffic flow End-to-end private connectivity without internet exposure Part 1: Retrieve Interface Endpoint DNS Information Before testing connectivity, you need to obtain the endpoint-specific DNS name that on-premises systems will use to reach S3.\nStep 1: Access VPC Endpoints Console\nOpen the Amazon VPC Console In the left navigation pane, click Endpoints You should see your recently created Interface Endpoint Step 2: Locate Endpoint DNS Names\nFind your endpoint:\nLook for the endpoint you created in the previous section It should have a name like S3-Interface-Endpoint-HybridAccess or similar Click on the Endpoint ID (format: vpce-xxxxxxxxx) to view details Access DNS configuration:\nIn the endpoint details page, scroll to the Details tab Look for the DNS names section Identify the Regional DNS Name:\nYou\u0026rsquo;ll see multiple DNS names listed Copy the first DNS name (the regional endpoint DNS name) Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Save this to a text editor - you\u0026rsquo;ll need it shortly DNS Name Types: Interface Endpoints provide several DNS names:\nRegional DNS name: Works across all AZs (recommended for general use) Zonal DNS names: Specific to each AZ where you deployed ENIs Private DNS name: Only works if you enabled private DNS (which we didn\u0026rsquo;t) For this test, use the regional DNS name as it provides automatic failover between AZs.\nUnderstanding the DNS Name Structure:\nThe regional DNS name follows this pattern:\nvpce-\u0026lt;endpoint-id\u0026gt;.s3.\u0026lt;region\u0026gt;.vpce.amazonaws.com This DNS name resolves to the private IP addresses of the Interface Endpoint ENIs deployed in your VPC subnets.\nPart 2: Connect to Simulated On-Premises Server Now you\u0026rsquo;ll connect to an EC2 instance that simulates an on-premises server in your corporate datacenter.\nStep 1: Open AWS Systems Manager Session Manager\nNavigate to Systems Manager:\nIn the AWS Console search bar at the top, type Session Manager Select Session Manager from the results (under Systems Manager) Access Session Manager:\nYou\u0026rsquo;ll be taken to the Session Manager start page This service provides secure shell access without requiring SSH keys or bastion hosts Session Manager Advantages: Session Manager provides secure browser-based shell access to EC2 instances without:\nOpening inbound SSH ports (improved security) Managing SSH keys or credentials Deploying bastion hosts Configuring security group SSH rules Perfect for enterprise environments with strict security requirements.\nStep 2: Start Interactive Session\nInitiate connection:\nClick the Start session button You\u0026rsquo;ll see a list of available EC2 instances Select the on-premises instance:\nLook for an instance named Test-Interface-Endpoint This EC2 instance runs in \u0026ldquo;VPC On-prem\u0026rdquo; subnet It simulates a server in your corporate datacenter Click the radio button next to this instance Start the session:\nClick Start session button at the bottom Session Manager will open a new browser tab with a shell prompt You should see: sh-4.2$ or similar prompt Instance Context: The Test-Interface-Endpoint EC2 instance is configured to:\nRun in the \u0026ldquo;VPC On-prem\u0026rdquo; private subnet Route traffic through the VPN Gateway you configured Use the Route 53 Outbound Resolver for DNS queries Simulate an on-premises application server accessing AWS services Part 3: Prepare Test Data for Upload You\u0026rsquo;ll now create a test file to upload to S3, demonstrating data transfer through the private endpoint.\nStep 1: Navigate to User Home Directory\nIn the Session Manager terminal, execute:\ncd ~ This changes to the ssm-user home directory where you have write permissions.\nStep 2: Create a Test File\nGenerate a 1GB test file to simulate a realistic data transfer:\nfallocate -l 1G onprem-data.dat Command Breakdown:\nfallocate: Efficiently creates a file of specified size -l 1G: Allocates 1 gigabyte of space onprem-data.dat: Output filename This creates a sparse file instantly without writing actual data (perfect for testing).\nStep 3: Verify File Creation\nConfirm the file was created:\nls -lh onprem-data.dat You should see output showing a 1.0G file.\nFile Size Selection: Using a 1GB file provides:\nMeasurable transfer time to observe network performance Sufficient size to verify throughput through VPN and endpoint Realistic representation of enterprise data transfers Quick enough for workshop purposes (full upload in seconds over AWS network) Part 4: Upload Data Through Interface Endpoint Now comes the critical test: uploading data to S3 using the Interface Endpoint you created.\nStep 1: Construct the Endpoint URL\nFor Interface Endpoints, you must specify a custom endpoint URL. The format is:\nhttps://bucket.\u0026lt;regional-dns-name\u0026gt; Build your URL:\nStart with: https://bucket. Append the regional DNS name you copied earlier Example result: https://bucket.vpce-0a1b2c3d4e5f.s3.us-east-1.vpce.amazonaws.com DNS Name Format: When copying the regional DNS name from the VPC console:\nDo NOT include the leading asterisk (*.) if shown Use only the portion: vpce-xxxxxxx.s3.us-east-1.vpce.amazonaws.com The bucket. prefix is added in the endpoint URL Step 2: Identify Your S3 Bucket\nYou need the name of the S3 bucket created earlier in section 5.3 (Gateway Endpoint testing):\nFormat: Something like my-endpoint-test-bucket-123456 If you don\u0026rsquo;t remember, check the S3 console in another browser tab Step 3: Execute S3 Upload Command\nIn the Session Manager terminal, run:\naws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; onprem-data.dat s3://\u0026lt;your-bucket-name\u0026gt; Replace placeholders:\n\u0026lt;Regional-DNS-Name\u0026gt;: Your actual endpoint DNS name (without asterisk) \u0026lt;your-bucket-name\u0026gt;: Your actual S3 bucket name Example with real values:\naws s3 cp --endpoint-url https://bucket.vpce-0a1b2c3d4e.s3.us-east-1.vpce.amazonaws.com onprem-data.dat s3://my-endpoint-test-bucket-123456 Understanding the Command:\naws s3 cp: AWS CLI command to copy files to/from S3 --endpoint-url: Critical parameter - directs the S3 API call to the Interface Endpoint instead of public S3 endpoint onprem-data.dat: Source file (local file on the instance) s3://\u0026lt;bucket-name\u0026gt;: Destination S3 bucket Why \u0026ndash;endpoint-url is Required: By default, AWS CLI uses the public S3 endpoint (s3.amazonaws.com). The --endpoint-url parameter overrides this to use your Interface Endpoint\u0026rsquo;s private DNS name, ensuring traffic routes through the VPN and private endpoint rather than the internet.\nStep 4: Monitor Upload Progress\nYou should see output similar to:\nupload: ./onprem-data.dat to s3://my-bucket/onprem-data.dat Completed 1.0 GiB/1.0 GiB (50.0 MiB/s) The upload should complete within seconds given the high-speed AWS backbone network.\nWhat Just Happened:\nBehind the scenes, your data traveled through this path:\nEC2 On-Prem Instance → Route Table Lookup → VPN Gateway EC2 → IPsec VPN Tunnel → Transit Gateway → VPC Cloud → Interface Endpoint ENI (Private IP) → AWS PrivateLink → S3 Service All traffic remained on the AWS private network - no internet exposure!\nPart 5: Verify Successful Upload in S3 Console Finally, confirm the data arrived successfully in your S3 bucket.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console You\u0026rsquo;ll see a list of all your S3 buckets Step 2: Access Your Bucket\nLocate your bucket:\nFind the bucket you used in the upload command Click on the bucket name to view its contents View objects:\nYou should now see the file onprem-data.dat listed Check the Size column - it should show 1.0 GB Note the Last modified timestamp - should be recent Step 3: Inspect Object Details (Optional)\nClick on the object name to view detailed properties:\nStorage class: Likely Standard Server-side encryption: May show default encryption Metadata: Any custom metadata added Tags: Any tags applied during upload Test Successful! Your on-premises simulated environment successfully uploaded data to S3 through:\nRoute 53 Outbound Resolver (DNS query from on-prem) VPN Tunnel (network connectivity) Route 53 Inbound Resolver (DNS resolution to private IP) Interface VPC Endpoint (private S3 access point) AWS PrivateLink (secure service connection) All traffic remained private without traversing the public internet!\nUnderstanding Your Test Results Connectivity Validation:\nThis successful test confirms:\nDNS Resolution Working:\nOn-prem instance queried Outbound Resolver Query forwarded through VPN to Inbound Resolver Endpoint DNS name resolved to Interface Endpoint private IP Network Routing Functional:\nVPN tunnel carrying traffic between on-prem and cloud VPCs Route tables directing traffic correctly Transit Gateway (if used) routing between VPCs Security Configuration Correct:\nSecurity groups allowing HTTPS traffic to endpoint ENIs Endpoint policy permitting S3 operations IAM permissions on EC2 instance allowing S3 access Interface Endpoint Operational:\nENIs receiving requests on private IPs PrivateLink routing requests to S3 service Responses returning through private path Traffic Flow Diagram:\n┌─────────────────┐ DNS Query ┌──────────────────┐ │ On-Prem EC2 │─────────────────\u0026gt;│ Outbound Resolver│ │ Instance │ │ (VPC On-prem) │ └────────┬────────┘ └────────┬─────────┘ │ │ │ Data Upload (HTTPS) │ DNS Forward │ │ ▼ ▼ ┌─────────────────┐ VPN Tunnel ┌──────────────────┐ │ VPN Gateway │◄────────────────►│ Inbound Resolver │ │ EC2 Instance │ │ (VPC Cloud) │ └────────┬────────┘ └────────┬─────────┘ │ │ │ │ DNS Response ▼ │ (Private IP) ┌─────────────────┐ │ │ Transit Gateway │ │ └────────┬────────┘ │ │ │ ▼ ▼ ┌─────────────────┐ ┌──────────────────┐ │ VPC Cloud │ │ Private Hosted │ │ Route Table │ │ Zone (Route 53) │ └────────┬────────┘ └──────────────────┘ │ ▼ ┌─────────────────────────────────┐ │ Interface Endpoint ENI │ │ (Private IP: 10.0.x.x) │ └────────┬────────────────────────┘ │ │ AWS PrivateLink ▼ ┌─────────────────────────────────┐ │ Amazon S3 Service │ │ (Backend in AWS Network) │ └─────────────────────────────────┘ Cleanup for Next Section (Optional) If you want to test again or prepare for the next workshop section:\nIn Session Manager terminal:\n# Remove the local test file to free space rm onprem-data.dat In S3 Console (optional):\nYou can leave the object in S3 or delete it The object will be cleaned up in the final workshop cleanup section Leave Infrastructure Running: Do NOT delete the Interface Endpoint, VPN configuration, or Route 53 Resolvers yet. These will be used in subsequent workshop sections and cleaned up at the end.\nKey Takeaways What You Accomplished:\n✅ Retrieved Interface Endpoint DNS names for API access configuration\n✅ Connected to simulated on-premises server using Session Manager\n✅ Created test data representing corporate files\n✅ Uploaded data to S3 through private Interface Endpoint\n✅ Verified successful transfer in S3 console\n✅ Validated end-to-end hybrid connectivity across multiple AWS services\nEnterprise Architecture Patterns Demonstrated:\nHybrid Cloud Connectivity: VPN-based connection between on-prem and AWS Private Service Access: Using VPC Endpoints to avoid internet routing DNS Resolution: Hybrid DNS architecture with Route 53 Resolver Secure Remote Access: Session Manager for bastion-less connectivity Defense in Depth: Multiple layers (VPN, security groups, endpoint policies) Production Considerations:\nFor real-world deployments, enhance this architecture with:\nAWS Direct Connect instead of VPN for dedicated network connection Multiple VPN tunnels for redundancy and higher availability VPC Flow Logs to monitor and audit traffic patterns CloudWatch metrics for endpoint performance monitoring Restrictive endpoint policies limiting access to specific buckets Private DNS enablement for automatic DNS resolution Multi-region endpoints for disaster recovery scenarios This test validates that your hybrid architecture is production-ready for private S3 access from on-premises systems!\n"},{"uri":"https://workshop-sample.fcjuni.com/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - IoT Weather Platform for Lab Research This proposal outlines the IoT Weather Platform designed for the ITea Lab team to collect and analyze real-time weather data. The solution uses Raspberry Pi/ESP32 edge devices to transmit data via MQTT to AWS IoT Core. The platform leverages AWS Serverless services, including S3 (Data Lake), AWS Glue (ETL), and an Amplify/Next.js web interface, secured by Amazon Cognito. The goal is to provide centralized monitoring, predictive analytics, and high cost efficiency (estimated at $0.7/month).\nBlog 2 - Enhancing Pinterest\u0026rsquo;s organizational security with a DNS firewall: Part 1 This blog introduces Pinterest\u0026rsquo;s approach to enhancing organizational security using a DNS Firewall, Part 1. It details why DNS traffic visibility is crucial for preventing data exfiltration, and how Route 53 Resolver Query Logs are collected and analyzed using Python to construct authoritative domain Allowlists. The article guides the steps to establish the foundation for the \u0026ldquo;Walled Garden\u0026rdquo; strategy and effectively control outbound (egress) network traffic.\nBlog 3 - Detecting Exceptional Traffic using CloudWatch Alarms and Lambda This blog instructs on how to automatically monitor applications and detect exceptional traffic/anomalies. You will learn why CloudWatch Alarms are important for real-time metric tracking, and how the Anomaly Detection feature, powered by ML, helps set flexible alerting thresholds. An AWS Lambda function is used as the action target to perform automated responses when an alarm triggers, such as sending enhanced notifications or gathering diagnostic data to reduce MTTR.\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/4.4-event4/","title":"Event 4: AWS Cloud Mastery Series #2 - DevOps on AWS","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nEvent Report “AWS Cloud Mastery Series #2: DevOps on AWS” Time and Theme Time: Monday, November 17, 2025 (8:30 AM – 5:00 PM) Theme: DevOps on AWS Event Objectives Provide a comprehensive understanding of the DevOps culture, principles, and tools on AWS. Guide attendees on building Continuous Integration and Continuous Deployment (CI/CD) Pipelines. Deep dive into Infrastructure as Code (IaC) and Containerization on AWS. Introduce services and best practices for Monitoring \u0026amp; Observability. List of Speakers Detailed speaker information is not available in the original data, assumed to be AWS Vietnam Solution Architects. Highlights of the Content 1. Morning Session: CI/CD Pipeline \u0026amp; IaC DevOps Mindset: Core principles and key metrics (DORA, MTTR). AWS CI/CD Services: CodeCommit (Source Control), CodeBuild (Build \u0026amp; Test), CodeDeploy (Deployment with Blue/Green, Canary), and CodePipeline (Orchestration). Infrastructure as Code (IaC): Comparison and demo of AWS CloudFormation and AWS CDK (Cloud Development Kit). 2. Afternoon Session: Container \u0026amp; Observability Container Services: Fundamentals of Docker and Microservices deployment using Amazon ECR, ECS \u0026amp; EKS. Monitoring \u0026amp; Observability: Using CloudWatch (Metrics, Logs, Alarms) and AWS X-Ray (Distributed tracing) to set up comprehensive system observability. Best Practices: Advanced deployment strategies (Feature flags, A/B testing) and Incident Management procedures. Key Takeaways DevOps and System Knowledge CI/CD Workflow: Clear understanding of how to fully automate the software release process, from code commit to production deployment. IaC Mindset: Grasping the benefits and application of IaC using both CloudFormation and CDK, enabling infrastructure management as code. Observability: Understanding the difference between traditional Monitoring and modern Observability, and knowing how to use tracing to troubleshoot issues in microservices architectures. Application to Work Infrastructure Automation: Apply CDK to write reusable IaC templates, simplifying the deployment of test environments. Pipeline Configuration: Set up a simple CI/CD Pipeline for a personal project, using the AWS Code suite to experience different deployment strategies (Blue/Green). Log/Metrics Analysis: Utilize CloudWatch to monitor performance and create alarms for developing applications. Event Experience Comprehensive Content: The full-day event offered rich content, covering almost all aspects of DevOps on AWS. Practical Demos: The demos on building Pipelines and deploying with IaC visually clarified complex concepts. Focus on Quality: The emphasis on DORA metrics and Incident Management highlighted the focus on delivering business value, not just simple automation. This event provided me with a solid foundation for building, deploying, and operating scalable applications on AWS.\n"},{"uri":"https://workshop-sample.fcjuni.com/4-eventparticipated/","title":"Events Attended","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy the content verbatim for your report, including this warning.\nIn this section, you need to list and describe in detail the events you have participated in during your internship or work period.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following information:\nEvent Name Date and Time Location (if applicable) Your role in the event (attendee, organizing support, speaker, etc.) A brief description of the content and main activities of the event Results or value gained (lessons learned, new skills, contribution to the team/project) This listing helps clearly demonstrate your actual participation, as well as the soft skills and experience you have accumulated through each event. During the internship, I participated in 3 events, with each event being a memorable experience with new, interesting, and valuable knowledge, along with wonderful gifts and moments.\nEvent 1 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nTime: Saturday, November 15, 2025 (8:30 AM – 12:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu St., Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in Event: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nTime: Monday, November 17, 2025 (8:30 AM – 5:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu St., Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in Event: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #3: Following the AWS Well-Architected Security Pillar\nTime: Saturday, November 29, 2025 (8:30 AM – 12:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu St., Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in Event: Attendee\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configuring Hybrid DNS for Private Endpoint Resolution","tags":[],"description":"","content":" ⚠️ Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will configure the DNS infrastructure to enable seamless name resolution for S3 API calls from your on-premises environment. This simulates how enterprise networks use conditional DNS forwarding to resolve AWS service endpoints to private IP addresses.\nWhy DNS Configuration Is Critical:\nAWS PrivateLink endpoints use elastic network interfaces (ENIs) with fixed private IP addresses in each Availability Zone. While these IPs remain stable for the endpoint\u0026rsquo;s lifecycle, AWS strongly recommends using DNS resolution rather than hardcoding IPs because:\nDynamic AZ Management: ENIs may be added to new AZs or removed over time Automatic Failover: DNS allows automatic switching between AZ-specific IPs during outages Service Updates: AWS may update endpoint infrastructure requiring new IPs Best Practice: DNS provides abstraction and flexibility for enterprise architectures What You\u0026rsquo;ll Configure:\nDNS Alias Records: Map S3 service domains to Interface Endpoint IPs in Route 53 Private Hosted Zone Resolver Forwarding Rule: Direct on-premises DNS queries for S3 to the cloud VPC End-to-End Testing: Validate DNS resolution and S3 access using the simulated on-premises environment This configuration mimics real-world hybrid DNS architectures where on-premises DNS servers conditionally forward specific domains to AWS for private resolution.\nPart 1: Create DNS Alias Records for Interface Endpoint You\u0026rsquo;ll now create DNS records in the Route 53 Private Hosted Zone that map S3 service names to your Interface Endpoint\u0026rsquo;s private IP addresses.\nUnderstanding the Private Hosted Zone:\nThe CloudFormation stack you deployed earlier created a Private Hosted Zone for s3.us-east-1.amazonaws.com. This zone:\nIs associated only with your VPCs (not public) Overrides public DNS resolution for S3 in your VPCs Allows you to return private IPs instead of public IPs for S3 domains Integrates with Route 53 Resolver for hybrid DNS Step 1: Access Route 53 Hosted Zones\nOpen the Route 53 Management Console (Hosted Zones section) You should see a Private Hosted Zone named s3.us-east-1.amazonaws.com Click on the hosted zone name to view its DNS records Private Hosted Zone Indicator: You\u0026rsquo;ll see \u0026ldquo;Private\u0026rdquo; listed under the Type column, and the Associated VPCs will show which VPCs use this zone for DNS resolution. This ensures only resources in your VPCs receive the private endpoint IPs.\nStep 2: Create Primary Alias Record\nYou\u0026rsquo;ll create an Alias record that points the base S3 domain to your Interface Endpoint.\nInitiate record creation:\nClick the Create record button at the top Configure the first record:\nRecord name: Leave blank (this creates a record for the zone apex: s3.us-east-1.amazonaws.com)\nRecord type: Select A – IPv4 address (default)\nAlias configuration:\nToggle the Alias switch to ON (enabled) This tells Route 53 to alias to another AWS resource Route traffic to:\nChoose Alias to VPC endpoint from the dropdown Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the Regional VPC Endpoint DNS name from your text editor This is the DNS name you saved in section 5.4.3 Format: vpce-xxxxxxxxx.s3.us-east-1.vpce.amazonaws.com Alias vs CNAME: Using Alias records (instead of CNAME) provides benefits:\nWorks at the zone apex (bare domain without subdomain) No charge for Alias queries to AWS resources Better performance with AWS service integration Automatic health checking and failover support Step 3: Create Wildcard Alias Record\nNow create a second record to handle all subdomains of the S3 service (e.g., bucket.s3.us-east-1.amazonaws.com).\nAdd another record:\nClick Add another record button (do not click Create yet) Configure the wildcard record:\nRecord name: Enter * (asterisk - this creates a wildcard)\nRecord type: Keep A – IPv4 address\nAlias configuration:\nToggle Alias to ON Route traffic to:\nChoose Alias to VPC endpoint Region:\nSelect US East (N. Virginia) [us-east-1] Choose endpoint:\nPaste the same Regional VPC Endpoint DNS name Create both records:\nClick Create records button Both records will be created simultaneously Why Two Records?\nThe two DNS records serve different purposes:\nApex Record (s3.us-east-1.amazonaws.com):\nHandles queries to the base S3 regional endpoint Used by commands like: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Wildcard Record (*.s3.us-east-1.amazonaws.com):\nHandles queries with bucket names in the subdomain Matches: bucket.s3.us-east-1.amazonaws.com, my-bucket.s3.us-east-1.amazonaws.com, etc. Used by path-style S3 URLs Step 4: Verify Record Creation\nAfter creation, you should see both records in the hosted zone:\nVerify:\nTwo new A records (apex and wildcard) Both show Type: A - Alias Value points to your VPC endpoint DNS Records Configured! These records ensure that when systems query for S3 domain names, they receive the private IP addresses of your Interface Endpoint ENIs instead of public S3 IPs.\nPart 2: Create Route 53 Resolver Forwarding Rule Now you\u0026rsquo;ll configure conditional DNS forwarding from the on-premises VPC to the cloud VPC. This simulates how on-premises DNS servers forward specific domains to AWS for resolution.\nUnderstanding Resolver Forwarding Rules:\nRoute 53 Resolver Forwarding Rules enable:\nConditional forwarding: Only specific domains are forwarded (e.g., s3.us-east-1.amazonaws.com) Hybrid DNS: On-premises systems query their local DNS, which forwards to AWS Centralized management: Rules define which domains resolve through AWS Bidirectional resolution: Can forward both from AWS to on-prem and vice versa In this workshop:\nOn-prem EC2 queries the Outbound Resolver in VPC On-prem Outbound Resolver forwards S3 queries through VPN to Inbound Resolver in VPC Cloud Inbound Resolver queries the Private Hosted Zone Response with private IPs travels back to on-prem Step 1: Retrieve Inbound Resolver IP Addresses\nFirst, you need the IP addresses where the Inbound Resolver is listening in VPC Cloud.\nNavigate to Inbound Endpoints:\nFrom Route 53 console, click Resolver in left sidebar Click Inbound endpoints Access endpoint details:\nClick on the Endpoint ID of the inbound endpoint CloudFormation created this with a name like VPCCloudInboundEndpoint Copy IP addresses: In the endpoint details, scroll to the IP addresses section You\u0026rsquo;ll see 2 IP addresses (one per AZ) Copy both IPs to your text editor Format: 10.0.x.x (within VPC Cloud CIDR) Multi-AZ Resolver: The Inbound Resolver has IPs in multiple AZs for high availability. The forwarding rule will use both, automatically failing over if one AZ becomes unavailable.\nStep 2: Create Forwarding Rule\nNavigate to Rules: In Route 53 console, click Resolver \u0026gt; Rules in left sidebar Click Create rule button Configure rule basics:\nName: Enter S3-PrivateEndpoint-ForwardingRule Description (optional): \u0026ldquo;Forward S3 DNS queries to VPC Cloud for private endpoint resolution\u0026rdquo; Rule type: Select Forward (not System) Domain name: Enter s3.us-east-1.amazonaws.com This domain name tells the rule: \u0026ldquo;Forward any DNS queries for this domain and its subdomains.\u0026rdquo;\nStep 3: Associate VPC and Outbound Endpoint\nVPC Configuration:\nVPCs to associate this rule with:\nSelect VPC On-prem from the dropdown This applies the rule to DNS queries originating in the on-premises VPC Outbound endpoint:\nSelect VPCOnpremOutboundEndpoint This is the resolver endpoint that forwards queries from VPC On-prem Outbound Endpoint Role: The Outbound Resolver in VPC On-prem receives DNS queries from EC2 instances and forwards them through the VPN tunnel to the target IP addresses (the Inbound Resolver in VPC Cloud).\nStep 4: Specify Target IP Addresses\nAdd target IPs:\nTarget IP addresses: This is where forwarded queries will be sent Click Add target IP address IP address: Paste the first Inbound Resolver IP from your text editor Click Add target IP address again IP address: Paste the second Inbound Resolver IP Both IPs should now be listed Create the rule:\nClick Submit button Understanding the Target IPs:\nThese are the Inbound Resolver IPs in VPC Cloud:\nThey listen for DNS queries coming through the VPN They query the Private Hosted Zone for answers They return results back through the VPN to the Outbound Resolver Step 5: Verify Rule Creation\nAfter creation, you should see your new forwarding rule:\nCheck that:\nStatus shows Active Type shows Forward Domain name is s3.us-east-1.amazonaws.com VPC shows VPC On-prem DNS Forwarding Configured! Queries from VPC On-prem for s3.us-east-1.amazonaws.com now forward through the Outbound Resolver, across the VPN tunnel, to the Inbound Resolver, and finally resolve through the Private Hosted Zone.\nPart 3: Test On-Premises DNS Resolution Now validate that the entire DNS resolution chain is working correctly.\nStep 1: Connect to On-Premises Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the Test-Interface-Endpoint EC2 instance (in VPC On-prem) Click Start session Step 2: Test DNS Resolution with Dig Command\nIn the Session Manager terminal, run:\ndig +short s3.us-east-1.amazonaws.com Expected Output:\nYou should see 2 IP addresses returned:\n10.0.1.100 10.0.2.100 (Your actual IPs will differ but should be in the VPC Cloud CIDR range)\nImportant Distinction: The IP addresses returned are the VPC Interface Endpoint ENI IPs, NOT the Inbound Resolver IPs you configured. Both sets of IPs are in the VPC Cloud CIDR block, but they serve different purposes:\nInbound Resolver IPs: Handle DNS queries (not returned in dig results) Interface Endpoint IPs: Where S3 API traffic is sent (returned by DNS queries) What Just Happened:\nThe dig command triggered this DNS resolution flow:\nEC2 On-prem → Outbound Resolver (VPC On-prem) → VPN Tunnel → Inbound Resolver (VPC Cloud) → Private Hosted Zone → Alias Record → Interface Endpoint IPs → Response back to EC2 Step 3: Verify IPs Match Interface Endpoint\nCross-check the IPs returned by dig against the actual Interface Endpoint:\nOpen VPC Console:\nNavigate to VPC \u0026gt; Endpoints Click on your S3 Interface Endpoint Check Subnets tab:\nClick the Subnets tab You\u0026rsquo;ll see ENI IDs and their associated private IPs Verify these IPs match those returned by the dig command The IPs should match perfectly, confirming DNS resolution is working correctly.\nDNS Resolution Validated! Your on-premises environment is successfully resolving S3 domain names to private Interface Endpoint IPs through the hybrid DNS architecture.\nStep 4: Test S3 API Access via DNS\nNow test that applications can use the standard S3 endpoint URL and automatically connect through the private endpoint.\nIn the Session Manager terminal, run:\naws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Expected Output:\nYou should see a list of your S3 buckets, including the test bucket created earlier.\nKey Observation:\nNotice you\u0026rsquo;re now using the standard S3 endpoint URL (https://s3.us-east-1.amazonaws.com), not the VPC endpoint-specific URL you used in section 5.4.3. This is possible because:\nDNS resolves s3.us-east-1.amazonaws.com to Interface Endpoint private IPs HTTPS traffic routes through VPN to the Interface Endpoint Applications don\u0026rsquo;t need to be aware of the private endpoint configuration Standard AWS SDKs and CLI work without modification This demonstrates the power of DNS-based private endpoint resolution!\nProduction Benefit: In enterprise environments, applications can use standard AWS service endpoints in their code. The DNS infrastructure automatically directs traffic through private endpoints when running on-premises or in VPCs, and through internet when running elsewhere (with appropriate routing).\nStep 5: Clean Up Session\nTerminate your Session Manager session by typing:\nexit Or close the browser tab\nUnderstanding Your Hybrid DNS Architecture Complete Traffic Flow:\nWhen an on-premises application accesses S3, this is the complete flow:\nDNS Resolution Path:\n┌──────────────────────┐ │ On-Prem Application │ │ Queries: s3.us-... │ └──────────┬───────────┘ │ ▼ ┌──────────────────────┐ │ EC2 Instance │ │ DNS Resolver Config │ └──────────┬───────────┘ │ ▼ ┌──────────────────────┐ │ Outbound Resolver │ │ (VPC On-prem) │ │ Forwarding Rule: │ │ s3.us-east-1.* │ └──────────┬───────────┘ │ │ VPN Tunnel ▼ ┌──────────────────────┐ │ Inbound Resolver │ │ (VPC Cloud) │ │ IPs: 10.0.x.x │ └──────────┬───────────┘ │ ▼ ┌──────────────────────┐ │ Private Hosted Zone │ │ s3.us-east-1.*.com │ │ Alias Records │ └──────────┬───────────┘ │ ▼ ┌──────────────────────┐ │ Returns: 10.0.1.100 │ │ 10.0.2.100 │ │ (Endpoint ENI IPs) │ └──────────────────────┘ Data Path (After DNS Resolution):\nOn-Prem Application → Interface Endpoint Private IP → VPN Tunnel → VPC Cloud → Interface Endpoint ENI → AWS PrivateLink → S3 Service Key Accomplishments What You\u0026rsquo;ve Built:\n✅ DNS Alias Records: Mapped S3 domains to Interface Endpoint private IPs\n✅ Conditional Forwarding: Configured forwarding rule for S3 domain from on-prem to cloud\n✅ Hybrid DNS Resolution: Enabled seamless DNS resolution across VPN\n✅ Transparent S3 Access: Applications use standard S3 endpoints with private routing\n✅ High Availability: Multi-AZ DNS and endpoint configuration for fault tolerance\nEnterprise Architecture Patterns:\nConditional DNS Forwarding: Split-brain DNS with domain-specific forwarding Private Hosted Zones: Override public DNS for AWS services with private IPs Alias Records: Efficient DNS routing to AWS resources Resolver Endpoints: Hybrid DNS bridge between on-prem and cloud Multi-AZ Resilience: Automatic failover for DNS and endpoints Production Enhancements:\nFor real-world deployments, consider:\nDNS caching strategies to reduce resolver query load Monitoring CloudWatch metrics for Resolver query counts and latency Multiple forwarding rules for different AWS services (EC2, RDS, etc.) DNS failback to public resolution if private endpoints are unavailable Integration with on-premises DNS servers (BIND, Active Directory, etc.) DNS query logging for security auditing and troubleshooting Route 53 Resolver Query Logging enabled for compliance This hybrid DNS architecture provides enterprise-grade private connectivity to AWS services while maintaining standard application code and AWS SDK usage!\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.5-policy/","title":"Implementing Fine-Grained Access Control with VPC Endpoint Policies","tags":[],"description":"","content":" ⚠️ Important: The information provided here is for reference purposes only. Please do not copy verbatim for your report including this warning message.\nIn this section, you will implement security best practices by applying restrictive VPC endpoint policies to control which S3 buckets can be accessed through your Gateway VPC Endpoint. This demonstrates the principle of least privilege in cloud architecture.\nUnderstanding VPC Endpoint Policies:\nVPC endpoint policies are IAM resource policies that provide granular access control for traffic flowing through VPC endpoints. They work as an additional security layer:\nDefault Behavior: When you create an endpoint without a custom policy, AWS applies a permissive default policy allowing full access to all resources of the target service Policy Type: JSON-based IAM resource policies attached directly to the endpoint Enforcement Point: Evaluated when traffic passes through the endpoint (not on the AWS service side) Combined Evaluation: Works in conjunction with IAM user/role policies and S3 bucket policies (all must allow an action) Scope: Can restrict access by resource (specific buckets), principal (specific IAM identities), or action (specific S3 operations) Why Use Endpoint Policies?\nEndpoint policies are critical for enterprise security architectures:\nDefense in Depth: Additional security layer beyond IAM and resource policies Network-Level Control: Restrict what can be accessed through specific network paths Compliance Requirements: Meet regulatory requirements for data access controls Prevent Data Exfiltration: Limit which buckets can be accessed from VPC resources Cost Optimization: Restrict access to authorized resources only, preventing misuse Workshop Scenario:\nYou\u0026rsquo;ll create a restrictive endpoint policy that:\nAllows access to a specific S3 bucket only Blocks access to all other S3 buckets Demonstrates how endpoint policies enforce access controls Part 1: Establish Baseline Connectivity Before applying a restrictive policy, verify current access to demonstrate the before/after comparison.\nStep 1: Connect to Test Instance\nOpen AWS Systems Manager \u0026gt; Session Manager Click Start session Select the EC2 instance named Test-Gateway-Endpoint This instance is in the VPC with the Gateway Endpoint It routes S3 traffic through the Gateway Endpoint via route table configuration Click Start session Step 2: Verify Access to Original S3 Bucket\nIn the Session Manager terminal, list the contents of the bucket you created in section 5.3:\naws s3 ls s3://\u0026lt;your-bucket-name\u0026gt; Replace \u0026lt;your-bucket-name\u0026gt; with your actual bucket name (e.g., my-endpoint-test-bucket-123456).\nExpected Output:\nYou should see the two 1GB test files uploaded in earlier sections:\n2024-12-07 10:30:15 1073741824 testfile.txt 2024-12-07 11:45:22 1073741824 onprem-data.dat Current Access: This works because the Gateway Endpoint currently has the default policy allowing full access (\u0026quot;*\u0026quot;) to all S3 resources. You\u0026rsquo;re about to change this to demonstrate policy-based restrictions.\nPart 2: Create Second S3 Bucket for Policy Testing You\u0026rsquo;ll create a second bucket to demonstrate selective access control.\nStep 1: Navigate to S3 Console\nOpen the Amazon S3 Console Click Create bucket Step 2: Configure New Bucket\nBucket name:\nFollow your existing naming pattern, but append -restricted Example: If your first bucket is my-endpoint-test-bucket-123456, name this one my-endpoint-test-bucket-123456-restricted Must be globally unique across all AWS accounts Region:\nEnsure it\u0026rsquo;s US East (N. Virginia) us-east-1 (same as your VPC and first bucket) Other settings:\nLeave all other settings at default values Default encryption: Enabled (SSE-S3) Block Public Access: Enabled (recommended) Versioning: Disabled (for workshop simplicity) Create bucket:\nScroll to bottom and click Create bucket Step 3: Verify Bucket Creation\nYou should see a success message and the new bucket listed in the S3 console.\nNaming Strategy: Using a consistent naming pattern (original name + suffix) makes it easy to identify related resources and simplifies policy ARN construction. In production, use naming conventions that reflect environment, application, and purpose.\nPart 3: Apply Restrictive Endpoint Policy Now you\u0026rsquo;ll modify the Gateway Endpoint policy to allow access only to the new bucket.\nStep 1: Navigate to VPC Endpoints\nOpen the VPC Console Click Endpoints in the left sidebar Locate and select the Gateway Endpoint you created in section 5.3 Type should show \u0026ldquo;Gateway\u0026rdquo; Service name should be com.amazonaws.us-east-1.s3 Step 2: Access Endpoint Policy Editor\nWith the endpoint selected, click the Policy tab in the bottom details pane You\u0026rsquo;ll see the current policy (default allows all) Click Edit policy button Current Default Policy:\nThe existing policy looks like this (allowing unrestricted access):\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 3: Apply Restrictive Policy\nClear the existing policy in the policy editor\nCopy and paste the following restrictive policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;RestrictToSpecificBucketPolicy\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAccessToRestrictedBucketOnly\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME\u0026#34;, \u0026#34;arn:aws:s3:::YOUR-RESTRICTED-BUCKET-NAME/*\u0026#34; ] } ] } Replace the ARN placeholders:\nFind YOUR-RESTRICTED-BUCKET-NAME (appears twice) Replace with your actual second bucket name (the one ending in -restricted) Example result: \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted\u0026#34;, \u0026#34;arn:aws:s3:::my-endpoint-test-bucket-123456-restricted/*\u0026#34; ] Save the policy:\nClick Save button Understanding the Policy:\nLet\u0026rsquo;s break down each component:\nVersion: IAM policy language version (always use \u0026ldquo;2012-10-17\u0026rdquo;) Id: Optional identifier for the policy Statement: Array of policy statements (can have multiple) Sid: Statement ID (descriptive label) Effect: Allow (grants permissions) or Deny (explicitly denies) Principal: \u0026quot;*\u0026quot; means any IAM identity (still subject to IAM policies) Action: s3:* allows all S3 operations (GetObject, PutObject, ListBucket, etc.) Resource: Two ARNs: First ARN: The bucket itself (for operations like ListBucket) Second ARN with /*: All objects within the bucket (for GetObject, PutObject, etc.) Policy Applied! The Gateway Endpoint now enforces that only the restricted bucket can be accessed through it. All attempts to access other buckets (including your original bucket) will be denied.\nPolicy Evaluation Logic:\nWhen an EC2 instance tries to access S3 through the endpoint:\n1. IAM policy on EC2 instance role: Must Allow 2. VPC Endpoint policy: Must Allow ← We just restricted this 3. S3 bucket policy: Must Allow (or not explicitly Deny) 4. S3 ACLs: Must Allow (or not explicitly Deny) Only if ALL checks pass → Access granted Part 4: Test Policy Enforcement Now validate that the endpoint policy is enforcing access restrictions.\nStep 1: Test Access to Original Bucket (Should Fail)\nIn your still-open Session Manager terminal on Test-Gateway-Endpoint, try to list the original bucket:\naws s3 ls s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nYou should receive an Access Denied error:\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied Why Access Denied? The VPC endpoint policy no longer includes the original bucket in its allowed resources. Even though your EC2 instance IAM role has S3 permissions, the network path (through the endpoint) blocks access.\nStep 2: Prepare Test Data\nCreate a test file to upload to the restricted bucket:\nNavigate to home directory:\ncd ~ Create a 1GB test file:\nfallocate -l 1G policy-test-file.dat Step 3: Test Access to Restricted Bucket (Should Succeed)\nUpload the file to the restricted bucket:\naws s3 cp policy-test-file.dat s3://\u0026lt;your-restricted-bucket-name\u0026gt; Replace \u0026lt;your-restricted-bucket-name\u0026gt; with your second bucket name.\nExpected Output:\nThe upload should succeed:\nupload: ./policy-test-file.dat to s3://my-bucket-restricted/policy-test-file.dat Completed 1.0 GiB/1.0 GiB Step 4: Verify File Upload in S3 Console\nGo back to the S3 Console Click on your restricted bucket You should see the policy-test-file.dat object Policy Enforcement Confirmed! The endpoint policy successfully:\n✅ Allowed access to the restricted bucket ❌ Blocked access to the original bucket This demonstrates fine-grained network-level access control.\nStep 5: Confirm Original Bucket Is Still Blocked\nAs a final test, try to upload to the original bucket (should fail):\naws s3 cp policy-test-file.dat s3://\u0026lt;your-original-bucket-name\u0026gt; Expected Output:\nAccess denied error:\nupload failed: ./policy-test-file.dat to s3://my-original-bucket/policy-test-file.dat An error occurred (AccessDenied) when calling the PutObject operation: Access Denied This confirms the endpoint policy is consistently enforcing restrictions.\nUnderstanding Policy Behavior Access Patterns Summary:\nOperation Target Result Reason List objects Original bucket ❌ Denied Not in endpoint policy Resource list Upload file Original bucket ❌ Denied Not in endpoint policy Resource list List objects Restricted bucket ✅ Allowed Explicitly allowed in endpoint policy Upload file Restricted bucket ✅ Allowed Explicitly allowed in endpoint policy How Traffic Flows:\nWhen EC2 accesses S3 through the Gateway Endpoint with a restrictive policy:\n┌─────────────────────┐ │ EC2 Instance │ │ Test-Gateway-EP │ └──────────┬──────────┘ │ │ S3 API Request ▼ ┌─────────────────────┐ │ Route Table │ │ Directs S3 traffic │ │ to Gateway Endpoint│ └──────────┬──────────┘ │ ▼ ┌─────────────────────────────────┐ │ Gateway VPC Endpoint │ │ Evaluates Endpoint Policy │ │ │ │ IF bucket IN allowed list: │ │ → Forward to S3 │ │ ELSE: │ │ → Return Access Denied │ └─────────────────────────────────┘ │ │ (If allowed) ▼ ┌─────────────────────┐ │ Amazon S3 Service │ │ (Backend) │ └─────────────────────┘ Key Observations:\nNetwork-Level Enforcement: The endpoint policy blocks traffic before it even reaches S3 Independent of IAM: Even if IAM policies allow access, endpoint policy can block it All-or-Nothing per Bucket: You cannot selectively allow only certain operations (e.g., read-only); the policy applies to all allowed actions Principal Agnostic: With \u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;, the restriction applies to all identities using the endpoint Production Use Cases for Endpoint Policies Enterprise Scenarios:\nData Residency Compliance:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::eu-compliant-data-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:ExistingObjectTag/DataClassification\u0026#34;: \u0026#34;EUPersonalData\u0026#34; } } }] } Prevent Data Exfiltration:\nWhitelist only approved corporate buckets Block access to personal or external buckets Audit endpoint policy changes with CloudTrail Multi-Tenant Environments:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::tenant-a-data/*\u0026#34;, \u0026#34;arn:aws:s3:::shared-resources/*\u0026#34; ] }] } Development vs Production Isolation:\nDev VPC endpoint: Access only to dev/test buckets Prod VPC endpoint: Access only to production buckets Prevents accidental cross-environment access Advanced Policy Patterns:\nCondition-Based Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::sensitive-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceVpce\u0026#34;: \u0026#34;vpce-xxxxxxxx\u0026#34; } } }] } Read-Only Access:\n{ \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::readonly-bucket\u0026#34;, \u0026#34;arn:aws:s3:::readonly-bucket/*\u0026#34; ] }] } Key Takeaways What You Accomplished:\n✅ Baseline Testing: Verified unrestricted access with default endpoint policy\n✅ Bucket Creation: Created second S3 bucket for policy testing\n✅ Policy Application: Applied restrictive endpoint policy limiting access to specific bucket\n✅ Enforcement Validation: Confirmed policy blocks unauthorized buckets and allows authorized bucket\n✅ Security Best Practice: Demonstrated principle of least privilege at network level\nSecurity Principles Demonstrated:\nDefense in Depth: Endpoint policies add network-level control layer Least Privilege: Grant only necessary access, not broad permissions Explicit Allow: Whitelist approach (specify what\u0026rsquo;s allowed, block everything else) Network Segmentation: Control data flow at VPC endpoint level Auditability: Policy changes logged in CloudTrail for compliance Production Recommendations:\nVersion Control: Store endpoint policies in Git with code review process Policy Testing: Test policies in dev/staging before production deployment Monitoring: Set up CloudWatch alarms for AccessDenied errors indicating policy violations Documentation: Maintain clear documentation of which buckets are accessible through which endpoints Regular Review: Audit endpoint policies quarterly to remove unnecessary access Condition Keys: Use advanced IAM condition keys for time-based, IP-based, or tag-based restrictions Deny Statements: Consider explicit Deny statements for high-security requirements This section demonstrated how VPC endpoint policies provide critical access control for enterprise cloud architectures, enabling you to enforce organizational security policies at the network infrastructure level!\n"},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://workshop-sample.fcjuni.com/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://workshop-sample.fcjuni.com/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at First Cloud Journey (FCJ) from 08/09/2025 to 28/11/2025, I had the opportunity to learn and practice AWS cloud services in a practical environment.\nI participated in learning core AWS services and built hands-on projects, through which I improved my skills in cloud computing, infrastructure design, serverless architecture, and technical documentation.\nThroughout the internship period, I maintained a consistent learning attitude, completed weekly tasks as planned, and actively explored AWS services to deepen my understanding beyond basic tutorials.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Improve practical troubleshooting skills when encountering complex AWS configuration issues Develop deeper understanding of cost optimization strategies beyond basic concepts Enhance ability to design complete architectures independently without reference materials Strengthen knowledge of security best practices and compliance requirements Build more confidence in explaining technical concepts to both technical and non-technical audiences "},{"uri":"https://workshop-sample.fcjuni.com/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://workshop-sample.fcjuni.com/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Objectives: Automated Scaling: Clearly understand the role and configuration of EC2 Auto Scaling Group (ASG) for automatic resource scaling (scaling out and in). Load Balancing: Master the architecture and operation of Elastic Load Balancer (ELB) (specifically ALB) for distributing traffic. Monitoring: Be proficient in using Amazon CloudWatch to collect metrics and logs, create Dashboards, and set up Alarms. Operational Automation: Practice establishing Scaling Policies based on monitored performance indicators. Planned Tasks for the Week: Day Task/Activity Focus Start Date Completion Date Resource Material Mon * Review and comprehend Auto Scaling Group (ASG) and Launch Template architecture. * Learn about different Load Balancer types (Classic, Application, Network). * Hands-on Practice: Create a Launch Template and configure a basic ASG. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ Tue * In-depth study of Application Load Balancer (ALB) and its components (Listener, Target Group, Health Check). * Hands-on Practice: + Create an ALB and Target Group. + Register the ASG\u0026rsquo;s EC2 Instances with the Target Group. 21/10/2025 21/10/2025 https://cloudjourney.awsstudygroup.com/ Wed * Learn about the Amazon CloudWatch monitoring service (Metrics, Logs, Events). * Hands-on Practice: + View EC2, ALB, and ASG Metrics in CloudWatch. + Create a CloudWatch Dashboard to track key indicators (CPU Utilization, Request Count). 22/10/2025 22/10/2025 https://cloudjourney.awsstudygroup.com/ Thu * Study CloudWatch Alarms and different Scaling Policy types (Target Tracking, Step Scaling). * Hands-on Practice: + Create a CloudWatch Alarm based on CPU Utilization (e.g., \u0026gt; 80%). + Configure a Scaling Policy for the ASG using the new Alarm (scale out on high CPU). 23/10/2025 23/10/2025 https://cloudjourney.awsstudygroup.com/ Fri * Testing \u0026amp; Resource Cleanup: * Hands-on Practice: + Verify the Scale Out/In process functions correctly. + Cleanup: Delete CloudWatch Alarms, delete the ASG (which automatically terminates EC2 instances), delete the ALB, and delete the Launch Template. 24/10/2025 24/10/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Scalability: Successfully deployed an application architecture capable of automatic scaling (Scale Out/In) via ASG and Launch Template. Load Balancing: Understood and configured the Application Load Balancer (ALB) to distribute traffic and perform effective Health Checks. Observability: Mastered the use of CloudWatch for collecting, visualizing (Dashboard), and monitoring resource health indicators. Automation: Successfully implemented an automated scaling mechanism based on performance (Scaling Policy and CloudWatch Alarms). Cost Management: Proficient in the cleanup procedure for costly resources such as ALB and ASG. "},{"uri":"https://workshop-sample.fcjuni.com/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://workshop-sample.fcjuni.com/tags/","title":"Tags","tags":[],"description":"","content":""}]