---
title: "Week 9 Worklog"
date: "`r Sys.Date()`"
weight: 1
chapter: false
pre: " <b> 1.9. </b> "
---
---
title: "Week 9 Worklog"
date: "`r Sys.Date()`"
weight: 1
chapter: false
pre: " <b> 1.9. </b> "
---

### Week 9 Objectives:

* Learn about serverless application development on AWS.
* Understand AWS Lambda and event-driven architecture.
* Build and deploy serverless applications.

### Tasks to be carried out this week:
| Day | Task                                                                                                                                                                                                   | Start Date | Completion Date | Reference Material                        |
| --- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------- | --------------- | ----------------------------------------- |
| 2   | - Introduction to Data & Analytics ecosystem on AWS <br> - Understand Data Lake concepts, ETL pipeline, and how to connect data from multiple sources | 03/11/2025 | 03/11/2025      | AWS Journey |
| 3   | - Create Data Lake on Amazon S3 <br> - Directory structure, access permissions <br> - Set up AWS Glue Crawler to identify data schema | 04/11/2025 | 04/11/2025      | AWS Journey |
| 4   | - Practice AWS Athena to query data in Data Lake <br> - Write basic SQL queries and export results to S3 | 05/11/2025 | 05/11/2025      | AWS Journey |
| 5   | - Introduction and practice with Amazon QuickSight <br> - Connect QuickSight with Athena to visualize data <br> - Create simple dashboard with charts and summary tables | 06/11/2025 | 06/11/2025      | AWS Journey |
| 6   | - Review & consolidate weekly knowledge: <br> + Data collection → processing → analysis process on AWS <br> + Compare Glue, Athena, QuickSight with traditional tools <br> + Write summary report of practice process | 07/11/2025 | 07/11/2025      | AWS Journey |


### Week 9 Achievements:

* Learned about serverless computing:
    * Understood serverless benefits (no server management, auto-scaling)
    * Learned AWS Lambda pricing model (pay per execution)
    * Studied serverless use cases

* Practiced with AWS Lambda:
    * Created Lambda functions in Python and Node.js
    * Configured function settings:
        * Memory: 128MB - 512MB
        * Timeout: 3 seconds - 30 seconds
        * Environment variables for configuration
    * Tested functions with sample events
    * Viewed execution logs in CloudWatch

* Worked with Lambda triggers:
    * Set up S3 trigger for file processing
    * Created EventBridge rule for scheduled execution
    * Configured API Gateway integration
    * Tested automatic function invocation

* Used AWS SAM:
    * Installed SAM CLI
    * Created SAM project with template.yaml
    * Defined Lambda functions and API Gateway in SAM
    * Deployed application with `sam deploy`
    * Tested local development with `sam local`

* Built serverless API:
    * Created REST API with API Gateway
    * Implemented CRUD operations with Lambda
    * Connected to DynamoDB for data storage
    * Tested API with Postman
    * Configured CORS for web access

* Integrated with DynamoDB:
    * Created DynamoDB table
    * Implemented Lambda functions to read/write data
    * Used boto3 (Python) for DynamoDB operations
    * Tested data persistence
    * Monitored DynamoDB metrics
      │  S3: Raw data, all formats              │
      └─────────────────────────────────────────┘
      ↓
      ┌─────────────────────────────────────────┐
      │  Processing Layer                       │
      │  - AWS Glue ETL Jobs                    │
      │  - Glue Data Quality                    │
      │  - EMR for complex transformations      │
      └─────────────────────────────────────────┘
      ↓
      ┌─────────────────────────────────────────┐
      │  Silver Layer (Cleansed Zone)           │
      │  S3: Parquet, validated, standardized   │
      └─────────────────────────────────────────┘
      ↓
      ┌─────────────────────────────────────────┐
      │  Aggregation Layer                      │
      │  - Glue ETL Jobs (aggregations)         │
      │  - Athena CTAS queries                  │
      └─────────────────────────────────────────┘
      ↓
      ┌─────────────────────────────────────────┐
      │  Gold Layer (Business Zone)             │
      │  S3: Optimized data marts               │
      └─────────────────────────────────────────┘
      ↓
      ┌─────────────────────────────────────────┐
      │  Consumption Layer                      │
      │  - Amazon Athena (SQL analytics)        │
      │  - QuickSight (BI dashboards)           │
      │  - SageMaker (ML models)                │
      │  - Redshift Spectrum (DW integration)   │
      └─────────────────────────────────────────┘
  ```

* **Data Governance with AWS Lake Formation:**
    * **Access Control:**
        - Centralized permissions management
        - Column-level security for PII fields
        - Row-level security based on user attributes
        - Tag-based access control (TBAC)
        - Cross-account sharing with AWS RAM

    * **Permissions Matrix:**
      | Role | Bronze | Silver | Gold | PII Columns |
      |------|--------|--------|------|-------------|
      | Data Engineers | Read/Write | Read/Write | Read/Write | Masked |
      | Data Analysts | Read | Read | Read | Masked |
      | Data Scientists | Read | Read | Read | Masked |
      | Business Users | No Access | No Access | Read | Denied |
      | Compliance Team | Read | Read | Read | Unmasked |

    * **Data Catalog:**
        - 347 tables cataloged
        - 1,245 columns documented
        - 89 data sources registered
        - Schema versioning enabled
        - Automated metadata extraction

#### 2. Enterprise Data Lake Implementation
* **S3 Data Lake Configuration:**
    * **Bucket Structure:**
      ```
      datalake-bronze-prod/
      ├── application-logs/
      │   └── year=2026/month=01/day=27/
      ├── database-cdc/
      │   └── source=postgresql/table=orders/
      ├── api-data/
      │   └── provider=stripe/endpoint=payments/
      ├── file-uploads/
      │   └── type=customer-data/
      └── iot-sensors/
          └── device-type=temperature/location=warehouse-1/
      
      datalake-silver-prod/
      ├── customers/
      │   └── year=2026/month=01/day=27/
      ├── orders/
      │   └── year=2026/month=01/day=27/
      ├── products/
      │   └── year=2026/month=01/
      └── transactions/
          └── year=2026/month=01/day=27/hour=10/
      
      datalake-gold-prod/
      ├── sales-analytics/
      │   └── report-date=2026-01-27/
      ├── customer-360/
      │   └── snapshot-date=2026-01-27/
      └── product-performance/
          └── analysis-period=2026-01/
      ```

    * **Security Configuration:**
        - Bucket encryption: SSE-KMS with automatic key rotation
        - Versioning: Enabled on all buckets
        - MFA Delete: Enabled for production buckets
        - Access logging: Enabled to audit bucket
        - Object Lock: Enabled for compliance data (WORM)
        - Block public access: Enforced at bucket and account level
        - VPC Endpoints: S3 access through private network only

    * **Cost Optimization:**
        - S3 Intelligent-Tiering: Automatic tiering for Bronze layer
        - Lifecycle policies:
            * Bronze: 30 days Standard → Glacier Instant Retrieval
            * Silver: 90 days Standard → Glacier Flexible Retrieval
            * Gold: 365 days Standard → Glacier Deep Archive
        - Compression: Parquet with Snappy (70% size reduction)
        - Partitioning: Reduced scan costs by 85%
        - S3 Select: Push-down filtering (60% cost reduction)
        - Storage costs: $450/month for 8.5 TB (down from $1,200 with Standard storage)

    * **Data Quality Metrics:**
        - Data freshness: 99.2% within SLA (15 minutes)
        - Data completeness: 98.7% (missing field validation)
        - Data accuracy: 99.5% (business rule validation)
        - Schema compliance: 100% (automated checks)
        - Duplicate rate: 0.3% (post-deduplication)

#### 3. AWS Glue ETL Pipeline Implementation
* **Glue Crawlers Configuration:**
    * **Bronze Crawler:**
        - Name: `bronze-data-crawler`
        - Schedule: Every 6 hours
        - Data sources: All Bronze layer S3 paths
        - Classifiers: JSON, CSV, Parquet, Avro custom classifiers
        - Tables created: 45 tables
        - Partitions discovered: 2,834 partitions
        - Schema inference: Automatic with conflict resolution

    * **Silver Crawler:**
        - Name: `silver-data-crawler`
        - Schedule: Daily at 2 AM
        - Data sources: Silver layer S3 paths
        - Partition projection: Enabled for time-based partitions
        - Tables created: 28 tables
        - Schema evolution: Track and version changes

* **Glue ETL Jobs:**

  **Job 1 - Bronze to Silver Transformation:**
  ```python
  import sys
  from awsglue.transforms import *
  from awsglue.utils import getResolvedOptions
  from pyspark.context import SparkContext
  from awsglue.context import GlueContext
  from awsglue.job import Job
  from awsglue.dynamicframe import DynamicFrame
  from pyspark.sql.functions import *
  
  args = getResolvedOptions(sys.argv, ['JOB_NAME', 'database_name', 'table_name'])
  
  sc = SparkContext()
  glueContext = GlueContext(sc)
  spark = glueContext.spark_session
  job = Job(glueContext)
  job.init(args['JOB_NAME'], args)
  
  # Read from Bronze layer
  bronze_dyf = glueContext.create_dynamic_frame.from_catalog(
      database=args['database_name'],
      table_name=args['table_name'],
      transformation_ctx="bronze_dyf"
  )
  
  # Convert to Spark DataFrame for transformations
  df = bronze_dyf.toDF()
  
  # Data Quality Checks
  df = df.filter(col("order_id").isNotNull())  # Remove null order IDs
  df = df.filter(col("order_amount") > 0)  # Valid amounts only
  df = df.dropDuplicates(["order_id"])  # Deduplicate
  
  # Data Transformations
  df = df.withColumn("order_date", to_date(col("created_at")))
  df = df.withColumn("order_amount", col("order_amount").cast("decimal(10,2)"))
  df = df.withColumn("customer_email", lower(trim(col("customer_email"))))
  
  # PII Masking
  df = df.withColumn("customer_phone_masked", 
                     regexp_replace(col("customer_phone"), "\\d{4}$", "XXXX"))
  
  # Add metadata columns
  df = df.withColumn("processing_timestamp", current_timestamp())
  df = df.withColumn("data_source", lit("order_system"))
  
  # Add partitioning columns
  df = df.withColumn("year", year(col("order_date")))
  df = df.withColumn("month", month(col("order_date")))
  df = df.withColumn("day", dayofmonth(col("order_date")))
  
  # Convert back to DynamicFrame
  silver_dyf = DynamicFrame.fromDF(df, glueContext, "silver_dyf")
  
  # Write to Silver layer in Parquet format with partitioning
  glueContext.write_dynamic_frame.from_options(
      frame=silver_dyf,
      connection_type="s3",
      connection_options={
          "path": "s3://datalake-silver-prod/orders/",
          "partitionKeys": ["year", "month", "day"]
      },
      format="parquet",
      format_options={
          "compression": "snappy"
      },
      transformation_ctx="silver_write"
  )
  
  job.commit()
  ```

  **Job 2 - Silver to Gold Aggregation:**
  ```python
  # Daily sales aggregation job
  from pyspark.sql.functions import sum, count, avg, max, min
  from pyspark.sql.window import Window
  
  # Read from Silver layer
  orders_df = spark.read.parquet("s3://datalake-silver-prod/orders/")
  customers_df = spark.read.parquet("s3://datalake-silver-prod/customers/")
  products_df = spark.read.parquet("s3://datalake-silver-prod/products/")
  
  # Join datasets
  sales_df = orders_df \
      .join(customers_df, "customer_id", "left") \
      .join(products_df, "product_id", "left")
  
  # Daily sales aggregation
  daily_sales = sales_df.groupBy("order_date", "product_category") \
      .agg(
          sum("order_amount").alias("total_revenue"),
          count("order_id").alias("total_orders"),
          avg("order_amount").alias("avg_order_value"),
          countDistinct("customer_id").alias("unique_customers")
      )
  
  # Calculate running totals with window functions
  window_spec = Window.partitionBy("product_category") \
      .orderBy("order_date") \
      .rowsBetween(Window.unboundedPreceding, Window.currentRow)
  
  daily_sales = daily_sales.withColumn(
      "cumulative_revenue",
      sum("total_revenue").over(window_spec)
  )
  
  # Write to Gold layer
  daily_sales.write \
      .mode("overwrite") \
      .partitionBy("order_date") \
      .parquet("s3://datalake-gold-prod/sales-analytics/daily-sales/")
  ```

* **Glue Data Quality Rules:**
  ```
  Rules = [
      # Completeness checks
      ColumnExists "order_id",
      ColumnExists "customer_id",
      ColumnExists "order_amount",
      
      # Uniqueness checks
      IsUnique "order_id",
      
      # Validity checks
      ColumnValues "order_amount" > 0,
      ColumnValues "order_status" in ["pending", "completed", "cancelled"],
      ColumnDataType "order_date" = "date",
      
      # Consistency checks
      ColumnLength "customer_email" <= 255,
      RowCount > 0,
      
      # Custom rules
      CustomSql "SELECT COUNT(*) FROM primary WHERE order_amount > 100000" = 0
  ]
  ```

* **Glue Job Monitoring:**
    - Job execution metrics tracked in CloudWatch
    - Average job duration: 8 minutes (processing 4.5GB data)
    - Success rate: 99.1%
    - Failed jobs: Auto-retry 3 times with exponential backoff
    - Cost per job run: $0.65 (2 DPU for 8 minutes)
    - Jobs triggered: 124 executions this week
    - Data processed: 562 GB total

#### 4. Advanced Analytics with Amazon Athena
* **Athena Workgroup Configuration:**
    * **Production Workgroup:**
        - Data scanned limit: 10 TB per month
        - Query timeout: 30 minutes
        - Results encryption: SSE-KMS
        - Results location: `s3://athena-results-prod/`
        - Results retention: 30 days
        - Engine version: Athena engine version 3
        - Cost allocation tags: Environment=Production, Team=Analytics

    * **Development Workgroup:**
        - Data scanned limit: 1 TB per month
        - Query timeout: 10 minutes
        - Enforce workgroup configuration
        - Query result reuse: 60 minutes

* **Advanced SQL Queries Examples:**

  **Query 1 - Customer Cohort Analysis:**
  ```sql
  WITH customer_cohorts AS (
    SELECT 
      customer_id,
      DATE_TRUNC('month', MIN(order_date)) AS cohort_month,
      DATE_TRUNC('month', order_date) AS order_month,
      SUM(order_amount) AS revenue
    FROM gold_layer.orders
    WHERE order_date >= DATE '2025-01-01'
    GROUP BY 1, 3
  ),
  cohort_metrics AS (
    SELECT 
      cohort_month,
      order_month,
      DATE_DIFF('month', cohort_month, order_month) AS months_since_first_order,
      COUNT(DISTINCT customer_id) AS customers,
      SUM(revenue) AS total_revenue
    FROM customer_cohorts
    GROUP BY 1, 2
  )
  SELECT 
    cohort_month,
    months_since_first_order,
    customers,
    total_revenue,
    ROUND(100.0 * customers / FIRST_VALUE(customers) 
      OVER (PARTITION BY cohort_month ORDER BY months_since_first_order), 2) 
      AS retention_rate
  FROM cohort_metrics
  ORDER BY cohort_month, months_since_first_order;
  ```

  **Query 2 - Product Affinity Analysis:**
  ```sql
  WITH product_pairs AS (
    SELECT 
      o1.order_id,
      o1.product_id AS product_a,
      o2.product_id AS product_b
    FROM gold_layer.order_items o1
    JOIN gold_layer.order_items o2 
      ON o1.order_id = o2.order_id 
      AND o1.product_id < o2.product_id
  )
  SELECT 
    pa.product_name AS product_a_name,
    pb.product_name AS product_b_name,
    COUNT(*) AS frequency,
    ROUND(100.0 * COUNT(*) / (
      SELECT COUNT(DISTINCT order_id) FROM gold_layer.order_items
    ), 2) AS support_percentage
  FROM product_pairs pp
  JOIN gold_layer.products pa ON pp.product_a = pa.product_id
  JOIN gold_layer.products pb ON pp.product_b = pb.product_id
  GROUP BY 1, 2
  HAVING COUNT(*) > 50
  ORDER BY frequency DESC
  LIMIT 20;
  ```

  **Query 3 - Time Series Forecasting Preparation:**
  ```sql
  WITH daily_metrics AS (
    SELECT 
      order_date,
      SUM(order_amount) AS daily_revenue,
      COUNT(DISTINCT order_id) AS daily_orders,
      COUNT(DISTINCT customer_id) AS daily_customers,
      AVG(order_amount) AS avg_order_value
    FROM gold_layer.orders
    WHERE order_date >= CURRENT_DATE - INTERVAL '365' DAY
    GROUP BY order_date
  ),
  moving_averages AS (
    SELECT 
      order_date,
      daily_revenue,
      AVG(daily_revenue) OVER (
        ORDER BY order_date 
        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
      ) AS ma_7day,
      AVG(daily_revenue) OVER (
        ORDER BY order_date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
      ) AS ma_30day,
      STDDEV(daily_revenue) OVER (
        ORDER BY order_date 
        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
      ) AS stddev_30day
    FROM daily_metrics
  )
  SELECT 
    order_date,
    daily_revenue,
    ma_7day,
    ma_30day,
    -- Z-score for anomaly detection
    (daily_revenue - ma_30day) / NULLIF(stddev_30day, 0) AS z_score,
    -- Day of week patterns
    day_of_week(order_date) AS day_of_week,
    -- Month of year seasonality
    month(order_date) AS month_num
  FROM moving_averages
  ORDER BY order_date DESC;
  ```

* **Query Performance Optimization:**
    - **Partitioning strategy:** Reduced scan by 85%
        * Before: 2.4 TB scanned per query
        * After: 360 GB scanned per query
    - **Columnar format (Parquet):** 70% compression ratio
    - **Partition projection:** Eliminated Glue Catalog calls
    - **CTAS (Create Table As Select):** Materialized frequent queries
    - **Bucketing:** Optimized join performance
    - **Query result reuse:** 60-minute caching
    - Average query cost: $0.18 (down from $1.20)

* **Athena Federated Queries:**
    - Connected data sources:
        * RDS PostgreSQL (operational database)
        * DynamoDB (user sessions)
        * DocumentDB (product catalog)
        * On-premises MySQL (via Lambda connector)
    - Cross-source analytics enabled
    - Query example: Join S3 data lake with live RDS data

#### 5. Enterprise BI with Amazon QuickSight
* **QuickSight Configuration:**
    * **Edition:** Enterprise (SAML SSO, row-level security, hourly refresh)
    * **Users:** 45 authors, 150 readers
    * **SPICE capacity:** 50 GB allocated
    * **Data sources connected:**
        - Amazon Athena (primary)
        - Amazon S3 (direct query)
        - Amazon RDS PostgreSQL
        - Salesforce (via connector)
        - Google Analytics (via connector)

* **Dashboard Implementations:**

  **Dashboard 1 - Executive KPI Dashboard:**
    * **Metrics displayed:**
        - Total Revenue (current vs previous period)
        - Active Customers (MoM growth %)
        - Average Order Value (trend)
        - Customer Lifetime Value
        - Churn Rate
        - Net Promoter Score (NPS)
    * **Visualizations:**
        - KPI cards with comparison indicators
        - Line charts for trends
        - Gauge charts for goal tracking
        - Heat map for regional performance
        - Funnel chart for conversion analysis
    * **Interactivity:**
        - Date range filters
        - Region/Category drill-downs
        - Export to PDF/Excel
        - Email subscriptions (daily/weekly)
    * **Auto-refresh:** Every 1 hour
    * **Users:** C-level executives (15 users)

  **Dashboard 2 - Sales Analytics Dashboard:**
    * **Analysis sections:**
        - Sales by product category (bar chart)
        - Geographic sales distribution (map)
        - Top 10 products by revenue (table with sparklines)
        - Sales vs target (combo chart)
        - Customer segmentation (tree map)
        - Month-over-month comparison (line chart)
    * **Calculated fields:**
      ```
      Revenue Growth % = 
        (sum({revenue}) - sumOver(sum({revenue}), [{period} ASC], 1, 1)) 
        / sumOver(sum({revenue}), [{period} ASC], 1, 1) * 100
      
      YTD Revenue = 
        sumOver(sum({revenue}), 
          [{year}], 
          [{year} = ${current_year}], 
          PRE_AGG)
      
      Customer Segment = 
        ifelse({lifetime_value} > 10000, "VIP",
          ifelse({lifetime_value} > 5000, "High Value",
            ifelse({lifetime_value} > 1000, "Medium Value", 
              "Low Value")))
      ```
    * **Parameters:**
        - Date range selector
        - Product category filter
        - Region multi-select
        - Minimum order value threshold
    * **Row-Level Security:**
        - Sales managers: See only their region
        - Product managers: See only their category
        - Analysts: See all data
    * **Users:** Sales team (35 users)

  **Dashboard 3 - Real-Time Operations Dashboard:**
    * **Live metrics (1-minute refresh):**
        - Current active users
        - Transactions per minute
        - System error rate
        - API response times
        - Inventory alerts
    * **Data source:** Kinesis Data Streams → Athena live query
    * **Alerts configured:**
        - Error rate > 2% → Email + Slack notification
        - Response time > 2s → PagerDuty alert
        - Inventory below threshold → Email to ops team
    * **Users:** Operations team (12 users)

* **QuickSight ML Insights:**
    - **Anomaly detection:** Automatically detect unusual patterns
        * Detected 3 anomalies this week:
            1. Unexpected revenue spike (holiday promotion - expected)
            2. Low traffic on Tuesday (system maintenance - known)
            3. Product category shift (new product launch - explained)
    - **Forecasting:** Revenue forecast for next 30 days
        * Predicted revenue: $1.2M ± $150K (confidence interval)
        * Accuracy on last month: 92%
    - **Contribution analysis:** Identify key drivers
        * Top revenue drivers: Product category (45%), Region (30%), Customer segment (15%)

* **Embedded Analytics:**
    - QuickSight dashboards embedded in internal web application
    - SSO integration with corporate IdP
    - Custom branding and white-labeling
    - API-based dashboard provisioning
    - Row-level security enforced
    - Usage: 2,500 dashboard views/week

#### 6. Streaming Analytics Implementation
* **Kinesis Data Streams Configuration:**
    * **Stream:** `clickstream-events`
    * **Shards:** 4 shards (4 MB/sec write, 8 MB/sec read capacity)
    * **Retention:** 7 days
    * **Encryption:** KMS encryption enabled
    * **Data producers:**
        - Web application (user clicks, page views)
        - Mobile app (app events, user actions)
        - IoT devices (sensor readings)
    * **Throughput:** 3.2 MB/sec average, 6.5 MB/sec peak
    * **Records/sec:** 850 average, 1,800 peak

* **Kinesis Data Firehose Delivery:**
    * **Delivery streams created:**
        1. **S3 Delivery Stream:**
            - Destination: `s3://datalake-bronze-prod/clickstream/`
            - Buffer size: 5 MB
            - Buffer interval: 300 seconds
            - Compression: Gzip
            - Format conversion: JSON to Parquet
            - Partitioning: year/month/day/hour
            - Data transformation: Lambda function for enrichment

        2. **OpenSearch Delivery Stream:**
            - Destination: OpenSearch domain `analytics-cluster`
            - Index: `clickstream-{yyyy-MM-dd}`
            - Buffer size: 5 MB
            - Buffer interval: 60 seconds
            - Retry duration: 3600 seconds
            - Use case: Real-time dashboards and search

* **Real-Time Processing with Lambda:**
  ```python
  import json
  import base64
  from datetime import datetime
  
  def lambda_handler(event, context):
      output = []
      
      for record in event['records']:
          # Decode Kinesis data
          payload = base64.b64decode(record['data']).decode('utf-8')
          data = json.loads(payload)
          
          # Enrich data
          data['processed_timestamp'] = datetime.utcnow().isoformat()
          data['partition_key'] = f"{data['user_id']}_{data['session_id']}"
          
          # Classify user action
          if 'purchase' in data.get('event_type', ''):
              data['event_category'] = 'conversion'
          elif 'add_to_cart' in data.get('event_type', ''):
              data['event_category'] = 'engagement'
          else:
              data['event_category'] = 'browsing'
          
          # Calculate session duration
          if data.get('event_type') == 'session_end':
              session_duration = data.get('timestamp') - data.get('session_start_time')
              data['session_duration_sec'] = session_duration
          
          # Encode back to base64
          output_record = {
              'recordId': record['recordId'],
              'result': 'Ok',
              'data': base64.b64encode(json.dumps(data).encode('utf-8')).decode('utf-8')
          }
          output.append(output_record)
      
      return {'records': output}
  ```

* **Kinesis Data Analytics Application:**
    * **Application type:** SQL-based analytics
    * **Use case:** Real-time aggregations and windowing
    * **SQL query example:**
      ```sql
      CREATE OR REPLACE STREAM "DESTINATION_SQL_STREAM" (
          window_start TIMESTAMP,
          event_type VARCHAR(50),
          event_count INTEGER,
          unique_users INTEGER,
          avg_session_duration DOUBLE
      );
      
      CREATE OR REPLACE PUMP "STREAM_PUMP" AS 
      INSERT INTO "DESTINATION_SQL_STREAM"
      SELECT STREAM 
          STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE) AS window_start,
          "event_type",
          COUNT(*) AS event_count,
          COUNT(DISTINCT "user_id") AS unique_users,
          AVG("session_duration_sec") AS avg_session_duration
      FROM "SOURCE_SQL_STREAM_001"
      GROUP BY 
          STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE),
          "event_type";
      ```
    * **Output:** Send aggregated data to:
        - Kinesis Data Streams (for further processing)
        - Lambda (for alerting)
        - OpenSearch (for real-time dashboards)

* **Streaming Analytics Metrics:**
    - Events processed: 2.4M events/day
    - Average latency: 1.2 seconds (ingestion to dashboard)
    - Processing cost: $85/day ($2,550/month)
    - Data volume: 3.8 GB/day compressed

#### 7. Comprehensive Monitoring and Operations
* **CloudWatch Dashboards:**
    * **Data Pipeline Health Dashboard:**
        - Glue job success/failure rates
        - ETL job durations
        - Data quality check pass rates
        - S3 bucket storage growth
        - Athena query metrics
        - Kinesis stream throughput

    * **Cost Monitoring Dashboard:**
        - S3 storage costs by layer
        - Glue job execution costs
        - Athena query costs by workgroup
        - Kinesis streaming costs
        - QuickSight user costs
        - Daily cost breakdown

* **CloudWatch Alarms:**
    - Glue job failures → SNS → Email + Slack
    - Athena cost threshold exceeded → Budget alert
    - Kinesis stream throttling → Auto-scaling trigger
    - Data freshness SLA breach → PagerDuty
    - S3 storage > 10 TB → Cost review notification

* **Data Lineage and Metadata:**
    - AWS Glue Data Catalog: Source of truth for schemas
    - Table metadata: 347 tables documented
    - Column-level metadata: Data types, descriptions, PII tags
    - Lineage tracking: Source → Bronze → Silver → Gold
    - Impact analysis: Which dashboards affected by schema changes

#### 8. Week 9 Summary and Key Metrics
* **Data Platform Metrics:**
  | Metric | Value |
  |--------|-------|
  | Total data ingested | 42 GB/day |
  | Data sources | 12 sources |
  | Tables in catalog | 347 tables |
  | Glue jobs | 18 jobs |
  | Athena queries/day | 450 queries |
  | QuickSight dashboards | 15 dashboards |
  | QuickSight users | 195 users |
  | Dashboard views/week | 2,500 views |
  | Data freshness (avg) | 12 minutes |
  | Query performance (p95) | 8 seconds |
  | Monthly data platform cost | $3,200 |
  | Cost per GB processed | $0.07 |

* **Business Value Delivered:**
    - Reduced time-to-insight from 2 days to 15 minutes (99% improvement)
    - Self-service analytics: 85% of reports now self-generated by business users
    - Data democratization: 195 users with data access (up from 12 analysts)
    - Cost savings: $8,500/month vs traditional DW solution
    - Decision speed: Real-time dashboards enable faster business decisions

* **Technical Achievements:**
    - Implemented modern medallion architecture
    - 100% serverless data platform (no infrastructure management)
    - Scalable from GB to PB (tested up to 500 GB/day)
    - Comprehensive data governance with Lake Formation
    - Real-time and batch analytics coexistence
    - Advanced ML insights integrated

* **Lessons Learned:**
    1. **Partitioning strategy is critical** for query performance and cost
    2. **Parquet format** significantly reduces storage and query costs
    3. **Data quality checks** should be implemented early in the pipeline
    4. **Incremental processing** is more efficient than full reprocessing
    5. **SPICE in QuickSight** provides fast dashboard performance
    6. **Row-level security** essential for data governance
    7. **Athena workgroups** enable cost control and usage tracking
    8. **Streaming and batch** architectures complement each other

* **Next Steps:**
    - Implement ML models with SageMaker using Gold layer data
    - Add more data sources (social media, marketing platforms)
    - Implement data quality scorecards
    - Expand QuickSight embedded analytics
    - Deploy DataZone for data marketplace
    - Implement automated data cataloging with AI
